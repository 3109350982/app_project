# services/customer_acquisition.py
"""
è·å®¢æœåŠ¡ï¼ˆä¿®å¤IPæå–å’Œè¯„è®ºé‡‡é›†é—®é¢˜ + æ•´é¡µä¸æ»‘æ»‘åŠ¨ç‰ˆæœ¬ï¼‰
"""
import asyncio
import json
import re
from urllib.parse import quote, urljoin
from services.base_service import BaseService
from core.config_manager import config_manager
from utils.data_storage import data_storage
import random
import time
class CustomerAcquisitionService(BaseService):
    """è·å®¢æœåŠ¡ï¼ˆä¿®å¤ç‰ˆ + æ•´é¡µä¸æ»‘æ»‘åŠ¨ï¼‰"""

    def __init__(self):
        super().__init__()
        self.api_comments_cache = {}

    async def execute(
        self,
        keywords=None,
        ip_keywords=None,
        sort_type="è§†é¢‘",
        videos_per_keyword=5,
        duration_minutes=10,
        videos=None,
        user_comment_keywords=None,
        **kwargs,
    ):
        """
        Args:
            keywords: é˜¶æ®µä¸€-å†…å®¹å…³é”®è¯åˆ—è¡¨
            user_comment_keywords: é˜¶æ®µäºŒ-ç”¨æˆ·è¯„è®ºå…³é”®è¯åˆ—è¡¨
            ip_keywords: é˜¶æ®µäºŒ-IPå…³é”®è¯åˆ—è¡¨
        """
        # å…³é”®è¯è§„èŒƒåŒ–å¤„ç†
        self.current_stage = kwargs.get("mode")
        user_comment_keys = self._normalize_keywords(user_comment_keywords)
        ip_keys = self._normalize_keywords(ip_keywords)
        kw_list = self._normalize_keywords(keywords)

        if not await self._ensure_browser_ready():
            await self._emit_event("error", "âŒ æµè§ˆå™¨æœªå°±ç»ª")
            return

        browser_manager = await self._get_browser_manager()
        storage = await self._get_data_storage()


        try:
            # é˜¶æ®µäºŒï¼šåŸºäºè§†é¢‘åˆ—è¡¨é‡‡é›†è¯„è®ºç”¨æˆ·
            if videos:
                if not user_comment_keys:
                    msg = "âŒ é˜¶æ®µäºŒéœ€è¦å¡«å†™ç”¨æˆ·è¯„è®ºå…³é”®è¯"
                    await self._emit_event("error", msg)
                    raise ValueError(msg)

                await self._emit_event("operation", f"ğŸš€ é˜¶æ®µäºŒå¼€å§‹ï¼šå…± {len(videos)} ä¸ªè§†é¢‘")
                await self._emit_event(
                    "operation",
                    f"ğŸ’¬ ç”¨æˆ·è¯„è®ºå…³é”®è¯: {' '.join(user_comment_keys)} | ğŸ“ IPå…³é”®è¯: {'ä»»æ„' if not ip_keys else ' '.join(ip_keys)}",
                )
                self.current_stage = "stage2"
                total_users = 0
                for i, url in enumerate(videos):
                    if await self._check_stop():
                        break
                    
                    self.api_comments_cache = {}
                    
                    await self._emit_event("operation", f"ğŸ“¹ å¤„ç†è§†é¢‘ {i+1}/{len(videos)}: {url}")
                    try:
                        await browser_manager.page.goto(url, wait_until="domcontentloaded")
                        await self._quick_wait_for_page_load(browser_manager)
                        
                        video_desc = await self._read_video_desc_enhanced(browser_manager)

                        users = await self._collect_comments_smooth_scroll(
                            browser_manager=browser_manager,
                            storage=storage,
                            user_comment_keywords=user_comment_keys,
                            ip_keywords=ip_keys,
                            video_url=url,
                            video_desc=video_desc,
                        )
                        total_users += len(users)
                        await self._emit_event(
                            "operation", f"âœ… æœ¬è§†é¢‘é‡‡é›† {len(users)} ä¸ªç”¨æˆ·"
                        )
                    except Exception as e:
                        await self._emit_event("error", f"âŒ å¤„ç†è§†é¢‘å¤±è´¥: {e}")
                        continue

                await self._emit_event("finished", f"ğŸ é˜¶æ®µäºŒå®Œæˆï¼šå…±é‡‡é›† {total_users} ä¸ªç”¨æˆ·")
                self.current_stage = None
                return

            # é˜¶æ®µä¸€ï¼šå…³é”®è¯æœç´¢é‡‡é›†è§†é¢‘

            if not kw_list:
                msg = "âŒ è¯·è¾“å…¥å†…å®¹å…³é”®è¯"
                await self._emit_event("error", msg)
                raise ValueError(msg)
            self.current_stage = "stage1"
            await self._emit_event(
                "operation",
                f"ğŸš€ é˜¶æ®µä¸€å¼€å§‹ï¼šæ¯å…³é”®è¯é‡‡é›†å‰ {videos_per_keyword} æ¡è§†é¢‘",
            )

            total_videos = 0
            for idx, kw in enumerate(kw_list):
                if await self._check_stop():
                    break

                await self._emit_event(
                    "operation", f"ğŸ” æœç´¢å…³é”®è¯: {kw} ({idx+1}/{len(kw_list)})"
                )
                try:
                    if sort_type == "ç»¼åˆ":
                        # ç»¼åˆtabï¼šè¿›å…¥ç»¼åˆé¡µ + ç›‘å¬æ¥å£ + æ»šåŠ¨è§¦å‘åŠ è½½
                        search_url = f"https://www.douyin.com/search/{quote(kw)}?type=general"
                        await self._emit_event("operation", f"ğŸŒ å¯¼èˆªåˆ°: {search_url}")
                        await browser_manager.page.goto(search_url, wait_until='domcontentloaded', timeout=30000)
                        await self._quick_wait_for_page_load(browser_manager)

                        videos_collected = await self._collect_from_general_search(browser_manager, kw, int(videos_per_keyword))
                    else:
                        # é»˜è®¤â€œè§†é¢‘â€tabï¼šä¿æŒä½ åŸæœ‰é€»è¾‘
                        search_url = f"https://www.douyin.com/search/{quote(kw)}?type=video"
                        await self._emit_event("operation", f"ğŸŒ å¯¼èˆªåˆ°: {search_url}")
                        await browser_manager.page.goto(search_url, wait_until='domcontentloaded', timeout=30000)
                        await self._quick_wait_for_page_load(browser_manager)

                        videos_collected = await self._collect_videos_with_smooth_scroll(
                            browser_manager, kw, int(videos_per_keyword)
                        )
                    total_videos += len(videos_collected)
                    
                    if len(videos_collected) == 0:
                        await self._emit_event("warning", f"âš ï¸ å…³é”®è¯ '{kw}' æœªé‡‡é›†åˆ°ä»»ä½•è§†é¢‘")
                    else:
                        await self._emit_event(
                            "operation", f"âœ… å…³é”®è¯ '{kw}' å®Œæˆï¼Œé‡‡é›† {len(videos_collected)} æ¡è§†é¢‘"
                        )

                except Exception as e:
                    await self._emit_event("error", f"âŒ æœç´¢å¤±è´¥: {e}")
                    continue

            await self._emit_event("finished", f"ğŸ é˜¶æ®µä¸€å®Œæˆï¼šå…±é‡‡é›† {total_videos} æ¡è§†é¢‘")

        except Exception as e:
            await self._emit_event("error", f"âŒ è·å®¢æœåŠ¡æ‰§è¡Œå¼‚å¸¸: {e}")
            raise
        self.current_stage = None
    async def enrich_videos_details(self, video_urls):
        """
        æ ¹æ®è§†é¢‘è¯¦æƒ…é¡µè¡¥å…¨:
        - ç‚¹èµæ•°
        - è¯„è®ºæ•°
        - æ”¶è—æ•°
        - ä½œè€…æ˜µç§°
        - ä½œè€…ä¸»é¡µ
        - è§†é¢‘æ–‡æ¡ˆ

        ç”± /api/videos/enrich_details è°ƒç”¨ã€‚
        """
        if not video_urls:
            return

        # ç¡®è®¤æµè§ˆå™¨æ­£å¸¸
        if not await self._ensure_browser_ready():
            await self._emit_event("error", "âŒ æµè§ˆå™¨æœªå°±ç»ªï¼Œæ— æ³•è·å–è§†é¢‘è¯¦æƒ…")
            return

        browser_manager = await self._get_browser_manager()
        storage = await self._get_data_storage()

        total = len(video_urls)
        success = 0

        for idx, url in enumerate(video_urls):
            if await self._check_stop():
                break

            await self._emit_event("operation", f"ğŸ¯ è·å–è§†é¢‘è¯¦æƒ… {idx+1}/{total}: {url}")
            try:
                await browser_manager.page.goto(url, wait_until="domcontentloaded", timeout=30000)
                await self._quick_wait_for_page_load(browser_manager)

                detail = await self._extract_video_detail_from_page(browser_manager)
                if not detail:
                    await self._emit_event("warning", "âš ï¸ è¯¦æƒ…é¡µæœªè§£æå‡ºä»»ä½•æ•°æ®")
                    continue

                # è¡¥ä¸Š video_url å­—æ®µï¼Œæ–¹ä¾¿ save_video / æ›´æ–°
                detail["video_url"] = url

                # è¿™é‡Œç›´æ¥å¤ç”¨ä½ ç°æœ‰çš„ save_video é€»è¾‘ï¼Œè®©å®ƒæŒ‰ video_url åš upsert
                try:
                    storage.save_video(detail)
                except Exception:
                    # ä¸ºäº†å…¼å®¹ä½ ä¹‹å‰åœ¨å…¶ä»–åœ°æ–¹ from utils.data_storage import data_storage çš„å†™æ³•
                    from utils.data_storage import data_storage as _ds
                    _ds.save_video(detail)

                success += 1
                await self._emit_event(
                    "debug",
                    f"âœ… æ›´æ–°è¯¦æƒ…æˆåŠŸ: èµ={detail.get('like_count', 0)}, è¯„={detail.get('comment_count', 0)}, æ”¶è—={detail.get('collect_count', 0)}",
                )
            except Exception as e:
                await self._emit_event("error", f"âŒ è·å–è§†é¢‘è¯¦æƒ…å¤±è´¥: {e}")

        await self._emit_event("operation", f"ğŸ è§†é¢‘è¯¦æƒ…è¡¥å…¨ç»“æŸï¼ŒæˆåŠŸ {success}/{total} ä¸ª")

    def _normalize_keywords(self, keywords):
        """è§„èŒƒåŒ–å…³é”®è¯è¾“å…¥"""
        if not keywords:
            return []
        if isinstance(keywords, str):
            return [k.strip() for k in keywords.split() if k.strip()]
        elif isinstance(keywords, list):
            return [k.strip() for k in keywords if k.strip()]
        return []
    def _format_time_ago_from_epoch(self, ts: int) -> str:
        try:
            import time
            delta = max(0, int(time.time()) - int(ts))
            if delta < 60:
                return "åˆšåˆš"
            mins = delta // 60
            if mins < 60:
                return f"{mins}åˆ†é’Ÿå‰"
            hours = mins // 60
            if hours < 24:
                return f"{hours}å°æ—¶å‰"
            days = hours // 24
            if days < 7:
                return f"{days}å¤©å‰"
            weeks = days // 7
            if days < 30:
                return f"{weeks}å‘¨å‰"
            months = days // 30
            if days < 365:
                return f"{months}æœˆå‰"
            years = days // 365
            return f"{years}å¹´å‰"
        except Exception:
            return ""
    def _parse_count_text(self, text: str) -> int:
        """
        è§£ææŠ–éŸ³è®¡æ•°æ–‡æœ¬ï¼Œä¾‹å¦‚:
        753
        3.9ä¸‡
        1.2äº¿
        """
        import re
        if not text:
            return 0

        text = text.strip().replace(" ", "")
        m = re.fullmatch(r"(\d+(?:\.\d+)?)([ä¸‡äº¿]?)", text)
        if not m:
            # éæ ‡å‡†æ ¼å¼æ—¶ï¼Œå°½é‡å–é‡Œé¢çš„æ•°å­—
            digits = re.findall(r"\d+", text)
            return int(digits[0]) if digits else 0

        num = float(m.group(1))
        unit = m.group(2)
        if unit == "ä¸‡":
            num *= 10000
        elif unit == "äº¿":
            num *= 100000000
        return int(num)

    def _split_comment_fields(self, raw_text, username_hint="", ip_hint=""):
        """
        ä¿®å¤ç‰ˆå­—æ®µæ‹†åˆ† - åŠ å¼ºIPå½’å±åœ°æå–
        """
        if not raw_text:
            return {"text": "", "ip_location": ip_hint,"comment_ago":""}
        
        text = raw_text.strip()
        ip_location = ip_hint
        time_ago = ""
        import re
        m = re.search(r'(åˆšåˆš)|(\d+\s*ä¸ª?(åˆ†é’Ÿ|å°æ™‚|å°æ—¶|å¤©|å‘¨|æœˆ|å¹´)å‰)', text)
        if m:
            time_ago = m.group(0)
        # å¦‚æœDOMä¸­æ²¡æœ‰æå–åˆ°IPï¼Œå°è¯•ä»æ–‡æœ¬ä¸­æå–IPå½’å±åœ°
        if not ip_location:
            ip_patterns = [
                r'(åˆšåˆš)|(\d+\s*(åˆ†é’Ÿ|å°æ™‚|å°æ—¶|å¤©|å‘¨|æœˆ|å¹´)å‰)[ï¼Œ,Â·]\s*([\u4e00-\u9fa5Â·\s]{2,15})',
                r'(åˆšåˆš)|(\d+\s*(åˆ†é’Ÿ|å°æ™‚|å°æ—¶|å¤©|å‘¨|æœˆ|å¹´)å‰)[Â·,ï¼Œ]\s*([\u4e00-\u9fa5Â·\s]{2,15})',
            ]

            for pattern in ip_patterns:
                match = re.search(pattern, text)
                if match:
                    groups = match.groups()
                    ip_candidate = None
                    # åœ¨æ‰€æœ‰åˆ†ç»„é‡ŒæŒ‘å‡ºâ€œä¸æ˜¯æ—¶é—´ã€è€Œæ˜¯ä¸­æ–‡åœ°åâ€çš„é‚£ä¸ª
                    for g in groups:
                        if not g:
                            continue
                        if re.search(r'(å‰|åˆ†é’Ÿ|å°æ™‚|å°æ—¶|å¤©|å‘¨|æœˆ|å¹´)', g):
                            continue
                        if re.search(r'[\u4e00-\u9fa5]', g):
                            ip_candidate = g.strip()
                            break
                    if not ip_candidate and groups:
                        ip_candidate = groups[-1].strip()

                    if ip_candidate:
                        ip_location = ip_candidate
                        text = re.sub(pattern, '', text).strip()
                        break
        
        # åªæ¸…ç†å›ºå®šå™ªå£°ï¼Œé¿å…åˆ é™¤æ­£æ–‡æ•°å­—
        noise_patterns = [
            r'å›å¤\s*\d*',
            r'åˆ†äº«\s*\d*', 
            r'ä¸¾æŠ¥',
            r'å±•å¼€\d*æ¡å›å¤',
            r'æŸ¥çœ‹\d*æ¡å›å¤',
        ]
        for pattern in noise_patterns:
            text = re.sub(pattern, '', text).strip()
        
        # ç§»é™¤æ—¶é—´ä¿¡æ¯ï¼ˆä¿ç•™æ ¼å¼åŒ¹é…ï¼‰
        time_patterns = [
            r'\d{1,2}æœˆ\d{1,2}æ—¥',
            r'\d{4}-\d{1,2}-\d{1,2}',
            r'\d{1,2}:\d{2}',
            r'\d+\s*ä¸ª?[åˆ†é’Ÿå°æ™‚å°æ—¶å¤©å‘¨æœˆå¹´å‰]?',   # ä¿ç•™â€œå‰/å¹´å‰â€å˜ä½“
            r'\d+\s*ä¸ª?[åˆ†é’Ÿå°æ™‚å°æ—¶å¤©å‘¨æœˆå¹´]å‰',
        ]
        for pattern in time_patterns:
            text = re.sub(pattern, '', text).strip()

        # å¦‚æœDOMä¸­æä¾›äº†ç”¨æˆ·åæç¤ºï¼Œå°è¯•ä»æ–‡æœ¬å¼€å¤´ç§»é™¤ç”¨æˆ·å
        if username_hint and text.startswith(username_hint):
            text = text[len(username_hint):].strip()
            text = re.sub(r'^[ï¼š:]\s*', '', text)
        
        # æ¸…ç†å¤šä½™çš„ç©ºç™½å­—ç¬¦
        text = re.sub(r'\s+', ' ', text).strip()
        
        return {
            "text": text,
            "ip_location": ip_location,
            "comment_ago": time_ago,
        }

    async def _collect_comments_smooth_scroll(self, browser_manager, storage, user_comment_keywords, ip_keywords, video_url, video_desc):
        """æ•´é¡µä¸æ»‘æ»‘åŠ¨ç‰ˆè¯„è®ºé‡‡é›† - æ·»åŠ æ—¶é—´æ§åˆ¶"""
        collected = []
        start_time = time.time()
        max_video_time = 600  # æ¯ä¸ªè§†é¢‘æœ€å¤š5åˆ†é’Ÿ
        try:
            # å¯åŠ¨æ¥å£ç›‘å¬
            await self._start_comment_api_listener(browser_manager)
            
            # ğŸ¯ æ•´é¡µä¸æ»‘æ»‘åŠ¨é‡‡é›†è¯„è®º
            await self._smooth_scroll_entire_page(browser_manager, max_scroll=500)
            # await self._click_all_reply_expand_buttons(browser_manager.page)
            # await self._smooth_scroll_entire_page(browser_manager, max_scroll=500)
            # æ–°å¢ï¼šæ»šåŠ¨è¯„è®ºåŒºåŸŸï¼Œè§¦å‘æ›´å¤šè¯„è®ºåŠ è½½
            #await self._scroll_comment_container(browser_manager.page)
            #await self._loop_scroll_comment_container(browser_manager.page, max_times=50)
            # ç‚¹å‡»æ‰€æœ‰å›å¤å±•å¼€æŒ‰é’®
            #await self._click_all_reply_expand_buttons(browser_manager.page)
            # æ»šåŠ¨æ‰€æœ‰æ¥¼ä¸­æ¥¼å®¹å™¨
            #await self._scroll_all_reply_containers(browser_manager.page, max_times=30)

            # æ£€æŸ¥æ˜¯å¦è¶…æ—¶
            # if time.time() - start_time > max_video_time:
            #     await self._emit_event("warning", f"â° è§†é¢‘é‡‡é›†è¶…æ—¶ï¼Œè·³è¿‡å‰©ä½™å¤„ç†")
            #     return collected
            # ä½¿ç”¨å¢å¼ºç‰ˆè¯„è®ºæå–
            comments = await self._extract_comments_enhanced(browser_manager)
            
            # æ•°æ®åˆå¹¶ - ä¼ å…¥video_url
            merged_comments = await self._merge_comment_data(comments, video_url)
            
            # å…³é”®è¯è¿‡æ»¤
            filtered_comments = await self._filter_comments_by_keywords(
                merged_comments, user_comment_keywords, ip_keywords
            )
            
            # ä¿å­˜ç”¨æˆ·æ•°æ®
            for comment in filtered_comments:
                cleaned_data = self._split_comment_fields(
                    comment['content'], 
                    comment['username'], 
                    comment.get('ip_location', '')
                )
                
                user_data = {
                    "username": comment['username'],
                    "user_url": comment['user_url'],
                    "comment_text": cleaned_data['text'],
                    "ip_location": cleaned_data['ip_location'],
                    "video_url": video_url,
                    "video_desc": video_desc,
                    "matched_keyword": " ".join(user_comment_keywords),
                    "comment_time": cleaned_data.get("comment_ago", ""),
                    "comment_ts": data_storage._parse_time_ago_to_epoch(cleaned_data.get("comment_ago", "")),
                }
                comment_time = comment.get('comment_time') or cleaned_data.get('comment_ago', '')
                comment_ts = comment.get('comment_ts') or data_storage._parse_time_ago_to_epoch(comment_time)

                user_data.update({
                    "comment_time": comment_time,
                    "comment_ts": int(comment_ts or 0),
                })

                if storage.save_user(user_data):
                    collected.append(user_data)
                    await self._emit_event(
                        "debug", f"ğŸ‘¤ é‡‡é›†ç”¨æˆ·: {comment['username']} | IP: {cleaned_data['ip_location']} | è¯„è®º: {cleaned_data['text'][:30]}..."
                    )
            
        except Exception as e:
            await self._emit_event("error", f"âŒ æ•´é¡µæ»‘åŠ¨è¯„è®ºé‡‡é›†å¤±è´¥: {e}")
            collected = await self._collect_comments_fallback(
                browser_manager, storage, user_comment_keywords, ip_keywords, video_url, video_desc
            )
        finally:
            await self._stop_comment_api_listener(browser_manager)
        
        return collected

    async def _smooth_scroll_entire_page(self, browser_manager, max_scroll=1000):  # ä»500å‡å°‘åˆ°100
        """åœ¨æ•´ä¸ªé¡µé¢è¿›è¡Œä¸æ»‘æ»‘åŠ¨ - ä¼˜åŒ–é€€å‡ºæ¡ä»¶"""
        page = browser_manager.page
        scroll_attempts = 0
        no_new_comments_count = 0
        last_comment_count = 0
        
        try:
            # è·å–åˆå§‹è¯„è®ºæ•°é‡
            last_comment_count = await self._get_comment_count(page)
            await self._emit_event("debug", f"ğŸ“Š åˆå§‹è¯„è®ºæ•°: {last_comment_count}")
            
            while scroll_attempts < max_scroll and no_new_comments_count < 10:  # ä»10å‡å°‘åˆ°5
                if await self._check_stop():
                    break
                
                # æ‰§è¡Œä¸æ»‘æ•´é¡µæ»šåŠ¨
                scroll_success = await self._execute_smooth_scroll(page, scroll_attempts)
                
                if not scroll_success:
                    break
                
                # ç­‰å¾…æ–°å†…å®¹åŠ è½½
                # æ£€æŸ¥æ˜¯å¦æœ‰æ–°è¯„è®º
                
                current_comment_count = await self._get_comment_count(page)
                
                if current_comment_count > last_comment_count:
                    new_comments = current_comment_count - last_comment_count
                    await self._emit_event("debug", f"ğŸ”„ æ»šåŠ¨ {scroll_attempts+1}: +{new_comments} æ¡æ–°è¯„è®º")
                    last_comment_count = current_comment_count
                    no_new_comments_count = 0
                else:
                    no_new_comments_count += 1
                    await self._emit_event("debug", f"â­ï¸ æ»šåŠ¨ {scroll_attempts+1}: æ— æ–°è¯„è®ºï¼ˆè¿ç»­ {no_new_comments_count} æ¬¡ï¼‰")
                
                scroll_attempts += 1
                
                # æ¯æ»šåŠ¨5æ¬¡éšæœºä¼‘æ¯ä¸€ä¸‹
                if scroll_attempts % 5 == 0:
                    await self.pause(0.1, 0.5, 'wheel_break')
            
            await self._emit_event("debug", f"ğŸ“œ æ•´é¡µæ»‘åŠ¨å®Œæˆï¼šå…±{scroll_attempts}æ¬¡ï¼Œè¯„è®º={last_comment_count}")
            
        except Exception as e:
            await self._emit_event("error", f"âŒ æ•´é¡µæ»‘åŠ¨å¤±è´¥: {e}")
    async def _scroll_comment_container(self, page):
        """æ»šåŠ¨è¯„è®ºå®¹å™¨ï¼Œè§¦å‘åŠ¨æ€åŠ è½½è¯„è®º"""
        try:
            await page.evaluate("""
                () => {
                    const cList = document.querySelector('[class*="comment-list"]');
                    if (cList) {
                        cList.scrollTop = cList.scrollHeight;
                    }
                }
            """)
        except Exception:
            pass

    async def _loop_scroll_comment_container(self, page, max_times=30):
        """å¾ªç¯æ»šåŠ¨è¯„è®ºå®¹å™¨ï¼ŒæŒç»­è§¦å‘è¯„è®ºåˆ†é¡µåŠ è½½"""
        try:
            for _ in range(max_times):
                # ç»ˆæ­¢æ¡ä»¶ï¼šå¤–éƒ¨å¼ºåˆ¶åœæ­¢
                if await self._check_stop():
                    break

                loaded_before = await self._get_comment_count(page)
                
                
                # æ»šåŠ¨è¯„è®ºå®¹å™¨åˆ°åº•éƒ¨
                await page.evaluate("""
                    () => {
                        const cList = document.querySelector('[class*="comment-list"]');
                        if (cList) {
                            cList.scrollTop = cList.scrollHeight;
                        }
                    }
                """)

                # ç­‰å¾…åŠ è½½å“åº”
                await asyncio.sleep(0.4)
                
                loaded_after = await self._get_comment_count(page)

                # å¦‚æœè¯„è®ºæ•°é‡æ²¡æœ‰å¢åŠ ï¼Œåˆ™åœæ­¢å¾ªç¯
                if loaded_after <= loaded_before:
                    break

        except Exception:
            pass
    async def _click_all_reply_expand_buttons(self, page):
        """ç‚¹å‡»æ‰€æœ‰â€œæŸ¥çœ‹å›å¤â€â€œå±•å¼€æ›´å¤šå›å¤â€æŒ‰é’®"""
        try:
            buttons = await page.query_selector_all('button')
            if not buttons:
                return

            for btn in buttons:
                try:
                    txt = await btn.inner_text()
                    if txt and ("æŸ¥çœ‹" in txt or "å›å¤" in txt or "å±•å¼€" in txt):
                        await btn.click()
                        #await asyncio.sleep(0.2)
                except:
                    continue
        except:
            pass
    async def _scroll_all_reply_containers(self, page, max_times=20):
        """æ»šåŠ¨æ‰€æœ‰æ¥¼ä¸­æ¥¼å®¹å™¨ï¼Œè§¦å‘å­è¯„è®ºåˆ†é¡µ"""
        try:
            for _ in range(max_times):
                containers = await page.query_selector_all('[class*="reply"]')
                if not containers:
                    break

                for c in containers:
                    try:
                        #await self._click_all_reply_expand_buttons(page)
                        await page.evaluate("""
                            el => { el.scrollTop = el.scrollHeight; }
                        """, c)
                    except:
                        continue

                await asyncio.sleep(0.2)
        except:
            pass

    async def _execute_smooth_scroll(self, page, attempt_number):
        """é¼ æ ‡æ»šè½®æ»šåŠ¨æ–¹æ¡ˆ"""
        try:
            # æ»šè½®è·ç¦»é…ç½® - æ›´æ¿€è¿›çš„èŒƒå›´
            wheel_configs = [
                {"distance": 2200, "description": "ä¸­ç­‰æ»šè½®"},
                {"distance": 2500, "description": "å¤§å¹…æ»šè½®"}, 
                {"distance": 2800, "description": "æ¿€è¿›æ»šè½®"},
                {"distance": 2000, "description": "ä¿å®ˆæ»šè½®"},  # å¶å°”ä¿å®ˆï¼Œå¢åŠ éšæœºæ€§
            ]
            
            # åŸºäºå°è¯•æ¬¡æ•°é€‰æ‹©ç­–ç•¥
            if attempt_number % 5 == 0:
                # æ¯5æ¬¡ä½¿ç”¨æœ€æ¿€è¿›çš„æ»šåŠ¨
                config = wheel_configs[2]
            elif attempt_number % 3 == 0:
                # æ¯3æ¬¡ä½¿ç”¨å¤§å¹…æ»šåŠ¨
                config = wheel_configs[1]
            elif random.random() < 0.2:
                # 20%æ¦‚ç‡ä½¿ç”¨ä¿å®ˆæ»šåŠ¨
                config = wheel_configs[3]
            else:
                # é»˜è®¤ä¸­ç­‰æ»šåŠ¨
                config = wheel_configs[0]
            
            distance = config['distance']
            
            # æ·»åŠ éšæœºå¾®å°åç§»ï¼Œæ¨¡æ‹Ÿäººç±»ä¸ç²¾ç¡®æ€§
            variance = random.randint(-100, 100)
            actual_distance = distance + variance
            
            # æ‰§è¡Œé¼ æ ‡æ»šè½®æ»šåŠ¨
            await page.evaluate(f"""
                () => {{
                    // è·å–å½“å‰æ»šåŠ¨ä½ç½®
                    const currentScroll = window.pageYOffset || document.documentElement.scrollTop;
                    const targetScroll = currentScroll + {actual_distance};
                    
                    console.log('ğŸ”„ å¼ºåˆ¶æ»šåŠ¨: ä»', currentScroll, 'åˆ°', targetScroll);
                    
                    // ç­–ç•¥1: ç›´æ¥è®¾ç½®æ»šåŠ¨ä½ç½®ï¼ˆæœ€å¼ºåˆ¶ï¼‰
                    window.scrollTo(0, targetScroll);
                    
                    // ç­–ç•¥2: è®¾ç½®æ–‡æ¡£å…ƒç´ çš„scrollTop
                    document.documentElement.scrollTop = targetScroll;
                    document.body.scrollTop = targetScroll;
                    
                    // ç­–ç•¥3: ä½¿ç”¨scrollByä½œä¸ºå¤‡ä»½
                    window.scrollBy(0, {actual_distance});
                    
                    // å¼ºåˆ¶è§¦å‘æ»šåŠ¨äº‹ä»¶ï¼ˆç»•è¿‡äº‹ä»¶é˜»æ­¢ï¼‰
                    const scrollEvent = new Event('scroll', {{ bubbles: true, cancelable: false }});
                    window.dispatchEvent(scrollEvent);
                    document.dispatchEvent(scrollEvent);
                    
                    // è§¦å‘resizeäº‹ä»¶ï¼ˆæœ‰æ—¶èƒ½è§¦å‘æ‡’åŠ è½½ï¼‰
                    const resizeEvent = new Event('resize', {{ bubbles: true }});
                    window.dispatchEvent(resizeEvent);
                    
                    // è§¦å‘wheeläº‹ä»¶ï¼ˆæ¨¡æ‹Ÿæ»šè½®ï¼‰
                    const wheelEvent = new WheelEvent('wheel', {{ 
                        bubbles: true, 
                        cancelable: true,
                        deltaY: {actual_distance}
                    }});
                    document.body.dispatchEvent(wheelEvent);
                }}
            """)
            # äººç±»åŒ–ç­‰å¾… - æ»šè½®éœ€è¦æ—¶é—´äº§ç”Ÿæ•ˆæœ
            await asyncio.sleep(random.uniform(0.2, 0.4))
            
            # å¶å°”æ·»åŠ å¾®å°å›æ»šï¼Œæ¨¡æ‹ŸçœŸå®ç”¨æˆ·è°ƒæ•´
            if random.random() < 0.15:
                await page.mouse.wheel(0, -random.randint(50, 150))
                await asyncio.sleep(0.3)
            
            return True
            
        except Exception as e:
            print(f"é¼ æ ‡æ»šè½®æ»šåŠ¨å¤±è´¥: {e}")
            # å¤±è´¥æ—¶é™çº§åˆ°JSæ»šåŠ¨
            try:
                await page.evaluate(f"window.scrollBy(0, 1000)")
                return True
            except:
                return False

    async def _get_comment_count(self, page):
        """è·å–å½“å‰è¯„è®ºæ•°é‡"""
        try:
            # å¤šç§é€‰æ‹©å™¨è·å–è¯„è®ºæ•°é‡
            selectors = [
                'div[data-e2e="comment-item"]',
                '[class*="comment-item"]',
                '.comment-item'
            ]
            
            for selector in selectors:
                try:
                    elements = await page.query_selector_all(selector)
                    if elements:
                        return len(elements)
                except:
                    continue
            return 0
        except:
            return 0
    async def _random_human_operation(self, page):
        """éšæœºäººç±»åŒ–æ“ä½œ"""
        operations = [
            lambda: page.evaluate("window.scrollBy({top: -100, left: 0, behavior: 'smooth'})"),  # å¶å°”å›æ»š
            lambda: page.mouse.move(random.randint(100, 500), random.randint(100, 300)),  # éšæœºç§»åŠ¨é¼ æ ‡
            lambda: asyncio.sleep(random.uniform(0.2, 0.4)),  # éšæœºæš‚åœ
        ]
        
        if random.random() < 0.3:  # 30%æ¦‚ç‡æ‰§è¡Œ
            op = random.choice(operations)
            try:
                if asyncio.iscoroutinefunction(op):
                    await op()
                else:
                    op()
            except:
                pass
    async def _collect_from_general_search(self, browser_manager, keyword: str, target_count: int) -> list[dict]:
        """ç»¼åˆtabé‡‡é›†ï¼šä¼˜å…ˆç›‘å¬æ¥å£ï¼Œå…¶æ¬¡ DOM å…œåº•ï¼Œå†æ¬¡ RENDER_DATA å…œåº•"""
        collected: list[dict] = []
        collected_ids: set[str] = set()
        stagnation = 0  # è¿ç»­æ— æ–°å¢è®¡æ•°

        await self._start_general_search_listener(browser_manager, keyword, collected, collected_ids)
        try:
            await self._emit_event("operation", f"ğŸ¬ ç»¼åˆæ–¹å¼å¼€å§‹é‡‡é›†ï¼Œç›®æ ‡: {target_count} ä¸ª")
            while len(collected) < target_count and not await self._check_stop():
                before = len(collected)

                # ä¸æ»‘æ»šåŠ¨ï¼Œè§¦å‘æ¥å£è¿”å›
                await browser_manager.page.evaluate(
                    "window.scrollBy({top: Math.floor(window.innerHeight*2.5), left: 0, behavior: 'smooth'})"
                )
                await self.pause(0.5, 0.8, 'scroll_load')

                # å¦‚æœæœ¬è½®æ²¡æœ‰æ–°å¢ï¼Œè§¦å‘ DOM å…œåº•
                if len(collected) == before:
                    dom_new = await self._dom_fallback_collect_general(browser_manager, keyword, collected_ids, collected)
                    if dom_new:
                        await self._emit_event("debug", f"ğŸ§© DOMå…œåº•æ–°å¢ {dom_new} æ¡")

                # ä»æ— æ–°å¢ï¼Œè§¦å‘ RENDER_DATA å…œåº•
                if len(collected) == before:
                    render_new = await self._render_data_fallback_collect(browser_manager, keyword, collected_ids, collected)
                    if render_new:
                        await self._emit_event("debug", f"ğŸ“¦ RENDER_DATAå…œåº•æ–°å¢ {render_new} æ¡")

                # æ— æ–°å¢è®¡æ•°
                if len(collected) == before:
                    stagnation += 1
                else:
                    stagnation = 0

                # è‹¥è¿ç»­å¤šè½®æ— æ–°å¢ï¼Œåˆ™è®¤ä¸ºå·²ç»é‡‡ç©º
                if stagnation >= 15:
                    await self._emit_event("debug", "ğŸ“œ ç»¼åˆæ–¹å¼ï¼šå¤šè½®æ— æ–°å¢ï¼Œåœæ­¢æ»šåŠ¨")
                    break
        finally:
            await self._stop_general_search_listener(browser_manager)

        return collected[:target_count]
    async def _start_general_search_listener(self, browser_manager, keyword: str, collected: list, collected_ids: set):
        async def handle_search_resp(response):
            try:
                url = (response.url or "").lower()
                if ("/aweme/v1/web/general/search/single" in url or
                    "/aweme/v1/web/general/search/patch" in url):
                    if response.status == 200:
                        data = await response.json()
                        items = self._extract_awemes_from_general_search(data)
                        new_count = 0
                        for it in items:
                            aweme_id = it.get("aweme_id")
                            if not aweme_id or aweme_id in collected_ids:
                                continue
                            video = {
                                "video_url": f"https://www.douyin.com/video/{aweme_id}",
                                "video_desc": it.get("desc", "") or "",
                                "keyword": keyword,
                                "publish_ts": int(it.get("create_time") or 0),
                                "publish_time": self._format_time_ago_from_epoch(int(it.get("create_time") or 0)) if it.get("create_time") else "",
                                # æ–°å¢ï¼šç›´æ¥ç”¨æ¥å£è§£æå¾—åˆ°çš„ä½œè€…/ç‚¹èµ
                                "author_name": it.get("author_name") or "",
                                "author_url": "",  # æ¥å£é‡Œé€šå¸¸æ²¡æœ‰ï¼Œç•™ç©ºï¼Œç¼ºäº†å†èµ° DOM å…œåº•
                                "like_count": int(it.get("like_count") or 0),
                            }
                            # â€”â€” è¡¥é½ä½œè€…ä¸ç‚¹èµï¼ˆåªåœ¨ç¼ºå­—æ®µæ—¶æ‰§è¡Œ DOM å…œåº•ï¼‰â€”â€”
                            if not video.get("author_name") or not video.get("like_count"):
                                try:
                                    extra = await browser_manager.page.evaluate("""
                                        (vid) => {
                                            const res = { author_name: '', author_url: '', like_count: 0 };

                                            // 1) å…ˆæŒ‰ aweme_id æ‰¾åˆ°å¯¹åº”è§†é¢‘å¡ç‰‡çš„ <a href="/video/<vid>">
                                            const anchors = Array.from(document.querySelectorAll('a[href*="/video/"]'));
                                            let target = null;
                                            for (const a of anchors) {
                                                const href = a.getAttribute('href') || '';
                                                if (href.includes('/video/') && href.includes(vid)) { target = a; break; }
                                            }
                                            if (!target) return res;

                                            // 2) å–å¤§å®¹å™¨ä½œä¸ºæœç´¢èŒƒå›´
                                            let card = target.closest('[class]') || target.parentElement;
                                            for (let i = 0; i < 5 && card && !card.querySelector; i++) {
                                                card = card.parentElement;
                                            }
                                            const root = card || document;

                                            // 3) ä½œè€…åã€ä¸»é¡µ
                                            const authorSpan = root.querySelector('span[class*="WUZSchd"]');
                                            if (authorSpan && authorSpan.textContent) res.author_name = authorSpan.textContent.trim();
                                            const authorLink = root.querySelector('a[href*="/user/"]');
                                            if (authorLink) res.author_url = authorLink.href;

                                            // 4) ç‚¹èµæ•°ï¼ˆç»¼åˆå¡ç‰‡çš„æ•°å€¼åœ¨ä¸€ä¸ª span é‡Œï¼‰
                                            const likeSpan = root.querySelector('div[class*="pMq5SQ1M"] span');
                                            if (likeSpan) {
                                                const t = (likeSpan.textContent || '').trim();
                                                const m = /([\\d\\.]+)\\s*[ä¸‡wW]?/.exec(t);
                                                if (m) {
                                                    const n = parseFloat(m[1]);
                                                    res.like_count = /ä¸‡|w|W/.test(t) ? Math.round(n * 10000) : Math.round(n);
                                                }
                                            }
                                            return res;
                                        }
                                    """, aweme_id) or {}
                                except Exception:
                                    extra = {}

                                # ä»…åœ¨ç¼ºå€¼æ—¶å†™å›
                                if not video.get("author_name") and extra.get("author_name"):
                                    video["author_name"] = extra["author_name"]
                                if not video.get("author_url") and extra.get("author_url"):
                                    video["author_url"] = extra["author_url"]
                                if not video.get("like_count") and extra.get("like_count"):
                                    video["like_count"] = int(extra["like_count"]) or 0


                            if data_storage.save_video(video):
                                collected.append(video)
                                collected_ids.add(aweme_id)
                                new_count += 1
                        if new_count:
                            await self._emit_event("debug", f"ğŸ“¡ ç»¼åˆæ¥å£æ–°å¢ {new_count} æ¡")
            except Exception as e:
                await self._emit_event("debug", f"âŒ è§£æç»¼åˆæœç´¢æ¥å£å¤±è´¥: {e}")

        self._general_search_handler = handle_search_resp
        # ä½ é¡¹ç›®å…¶ä»–åœ°æ–¹å¯¹ç›‘å¬æ˜¯ç”¨ page.onï¼Œè¿™é‡Œä¿æŒä¸€è‡´
        browser_manager.page.on("response", self._general_search_handler)

    async def _stop_general_search_listener(self, browser_manager):
        try:
            if hasattr(self, "_general_search_handler") and self._general_search_handler:
                # ä½ é¡¹ç›®é‡Œå·²ä½¿ç”¨ remove_listenerï¼Œè¿™é‡Œç”¨åŒæ ·æ–¹å¼
                browser_manager.page.remove_listener("response", self._general_search_handler)
        except Exception:
            pass
        finally:
            self._general_search_handler = None
    def _extract_awemes_from_general_search(self, data: dict) -> list[dict]:
        """ä»ç»¼åˆæœç´¢ JSON æå– aweme_id/desc/create_timeï¼ˆå¤šè·¯å¾„å…œåº•ï¼‰"""
        items: list[dict] = []
        try:
            candidates = []
            for path in (["data","data"], ["data","aweme_list"], ["data","mix_list"], ["list"], ["data"]):
                cur = data
                ok = True
                for k in path:
                    if isinstance(cur, dict) and k in cur:
                        cur = cur[k]
                    else:
                        ok = False; break
                if ok and isinstance(cur, list):
                    candidates.extend(cur)

            for c in candidates:
                aweme = (c.get("aweme") if isinstance(c, dict) else None) or \
                        (c.get("aweme_info") if isinstance(c, dict) else None) or \
                        (c.get("aweme_raw") if isinstance(c, dict) else None) or c

                if not isinstance(aweme, dict):
                    continue
                aweme_id = str(aweme.get("aweme_id") or aweme.get("awemeId") or "").strip()
                if not aweme_id:
                    continue

                desc = (aweme.get("desc") or aweme.get("description") or
                        (aweme.get("share_info") or {}).get("share_desc") or "")
                ct = aweme.get("create_time") or aweme.get("createTime") or 0

                # å–ä½œè€…åï¼ˆå¤šè·¯å¾„å…¼å®¹ï¼‰
                author_obj = aweme.get("author") or aweme.get("author_info") or aweme.get("authorInfo") or {}
                author_name = (author_obj.get("nickname") or author_obj.get("nickName") or "") or ""

                # å–ç‚¹èµæ•°ï¼ˆå¤šè·¯å¾„å…¼å®¹ï¼‰
                stats = aweme.get("statistics") or aweme.get("statisticsInfo") or {}
                digg = stats.get("digg_count") or stats.get("diggCount") or 0
                try:
                    like_count = int(digg)
                except Exception:
                    like_count = 0

                items.append({
                    "aweme_id": aweme_id,
                    "desc": desc or "",
                    "create_time": int(ct) if str(ct).isdigit() else 0,
                    "author_name": author_name,
                    "like_count": like_count
                })
        except Exception:
            pass
        return items
    async def _dom_fallback_collect_general(self, browser_manager, keyword: str, collected_ids: set, collected: list) -> int:
        """DOM å…œåº•ï¼šåœ¨ç»¼åˆé¡µæŠ“å–å¯è§å¡ç‰‡ä¸­çš„ video é“¾æ¥æˆ– aweme_id"""
        new_added = 0
        try:
            # 1) ç›´æ¥æŠ“ a[href*="/video/"]
            hrefs = await browser_manager.page.eval_on_selector_all(
                'a[href*="/video/"]',
                "els => els.map(e => e.getAttribute('href'))"
            ) or []

            # 2) å¸¸è§å¡ç‰‡å®¹å™¨ä¸Šå¯èƒ½å«æœ‰ data-* é‡ŒåŸ‹çš„ idï¼Œæˆ– dataset.awemeId
            #    å–æ‰€æœ‰åŒ…å« 'aweme' / 'video' / 'id' çš„ dataset/å±æ€§å­—ç¬¦ä¸²ï¼Œå†ç”¨æ­£åˆ™æŠ  19ä½æ•°å­—
            data_texts = await browser_manager.page.evaluate("""
                () => {
                    const nodes = Array.from(document.querySelectorAll('.search-result-card,[data-e2e*="video"],[data-e2e*="card"]'));
                    const out = [];
                    for (const n of nodes) {
                        const ds = n.dataset || {};
                        const line = JSON.stringify(ds) + ' ' + (n.getAttribute('data-aweme-id')||'') + ' ' + (n.getAttribute('data-id')||'');
                        out.push(line);
                    }
                    return out;
                }
            """) or []

            import re
            id_from_attrs = []
            for s in data_texts:
                for m in re.finditer(r'(\d{16,21})', s):
                    id_from_attrs.append(m.group(1))

            # 3) ç»Ÿä¸€è½¬æˆæ ‡å‡† URL
            urls = []
            from urllib.parse import urljoin
            for h in hrefs:
                if not h: 
                    continue
                urls.append(urljoin("https://www.douyin.com", h))
            for vid in id_from_attrs:
                urls.append(f"https://www.douyin.com/video/{vid}")

            # 4) å»é‡ & å…¥åº“
            for url in dict.fromkeys(urls).keys():
                # ä» URL æå– aweme_id åšäºŒæ¬¡å»é‡
                m = re.search(r'/video/(\d{16,21})', url)
                if not m:
                    continue
                aweme_id = m.group(1)
                if aweme_id in collected_ids:
                    continue
                video = {
                    "video_url": url,
                    "video_desc": "",
                    "keyword": keyword,
                    "publish_ts": 0,
                    "publish_time": ""
                }
                # â€”â€” è¡¥é½ä½œè€…ä¸ç‚¹èµï¼ˆä»ç»¼åˆé¡µå¡ç‰‡ DOM ä¸Šå°±è¿‘è§£æï¼‰â€”â€”
                try:
                    extra = await browser_manager.page.evaluate("""
                        (vid) => {
                            const res = { author_name: '', author_url: '', like_count: 0 };
                            const anchors = Array.from(document.querySelectorAll('a[href*="/video/"]'));
                            let target = null;
                            for (const a of anchors) {
                                const href = a.getAttribute('href') || '';
                                if (href.includes('/video/') && href.includes(vid)) { target = a; break; }
                            }
                            if (!target) return res;
                            let card = target.closest('[class]') || target.parentElement;

                            const authorEl = Array.from(card.querySelectorAll('span, a')).find(el => {
                                const t = (el.textContent || '').trim();
                                return t.startsWith('@') && t.length <= 40;
                            });
                            if (authorEl) {
                                res.author_name = (authorEl.textContent || '').trim().replace(/^@/, '');
                                const au = authorEl.closest('a');
                                if (au && au.href) res.author_url = au.href;
                            }

                            function parseLike(t) {
                                t = (t || '').trim();
                                if (!t || t.includes(':')) return null;
                                const m = t.match(/^(\\d+(?:\\.\\d+)?)([ä¸‡äº¿]?)$/);
                                if (!m) return null;
                                let num = parseFloat(m[1]);
                                if (m[2] === 'ä¸‡') num *= 10000;
                                if (m[2] === 'äº¿') num *= 100000000;
                                return Math.round(num);
                            }
                            let cands = Array.from(card.querySelectorAll('svg + span, div svg + span, div svg ~ span'));
                            cands = cands.concat(Array.from(card.querySelectorAll('span')));
                            for (const el of cands) {
                                const v = parseLike(el.textContent || '');
                                if (v) { res.like_count = v; break; }
                            }
                            return res;
                        }
                    """, aweme_id)
                    if isinstance(extra, dict):
                        video["author_name"] = extra.get("author_name") or ""
                        video["author_url"]  = extra.get("author_url")  or ""
                        video["like_count"]  = int(extra.get("like_count") or 0)
                except Exception:
                    pass

                if data_storage.save_video(video):
                    collected.append(video)
                    collected_ids.add(aweme_id)
                    new_added += 1
        except Exception as e:
            await self._emit_event("debug", f"âŒ DOMå…œåº•å¼‚å¸¸: {e}")
        return new_added
    async def _render_data_fallback_collect(self, browser_manager, keyword: str, collected_ids: set, collected: list) -> int:
        """RENDER_DATA å…œåº•ï¼šè§£æé¡µé¢å†…åµŒ JSON ä¸­çš„ aweme_id"""
        new_added = 0
        try:
            raw = await browser_manager.page.evaluate("""
                () => {
                    const el = document.querySelector('#RENDER_DATA');
                    return el ? el.textContent || el.innerText || '' : '';
                }
            """) or ""
            if not raw:
                return 0

            # RENDER_DATA é€šå¸¸æ˜¯ URL ç¼–ç è¿‡çš„
            from urllib.parse import unquote
            txt = unquote(raw)

            # æŠ  aweme_idï¼ˆ19~21ä½æ•°å­—ï¼‰ï¼Œå¹¶é¡ºä¾¿å– desc / create_timeï¼ˆå¯é€‰ï¼‰
            import re, json
            ids = set(re.findall(r'"aweme_id"\s*:\s*"?(\\d{16,21})"?', txt))
            # å°è¯•æŠŠ JSON è§£æå‡ºæ¥ä»¥å–æ›´å¤šå­—æ®µï¼ˆå¤±è´¥ä¹Ÿæ— å¦¨ï¼‰
            desc_map = {}
            ct_map = {}
            try:
                data = json.loads(txt)
                def deep_walk(o):
                    if isinstance(o, dict):
                        # å…¼å®¹å¤šå‘½å
                        aid = str(o.get("aweme_id") or o.get("awemeId") or "") if ("aweme_id" in o or "awemeId" in o) else ""
                        if aid:
                            if "desc" in o: desc_map[aid] = o.get("desc") or ""
                            if "create_time" in o: ct_map[aid] = int(o.get("create_time") or 0)
                            if "createTime" in o: ct_map[aid] = int(o.get("createTime") or 0)
                        for v in o.values(): deep_walk(v)
                    elif isinstance(o, list):
                        for v in o: deep_walk(v)
                deep_walk(data)
            except Exception:
                pass

            for aid in ids:
                if aid in collected_ids:
                    continue
                video = {
                    "video_url": f"https://www.douyin.com/video/{aid}",
                    "video_desc": desc_map.get(aid, ""),
                    "keyword": keyword,
                    "publish_ts": ct_map.get(aid, 0),
                    "publish_time": self._format_time_ago_from_epoch(ct_map[aid]) if aid in ct_map else ""
                }
                # â€”â€” è¡¥é½ä½œè€…ä¸ç‚¹èµï¼ˆä»ç»¼åˆé¡µå¡ç‰‡ DOM ä¸Šå°±è¿‘è§£æï¼‰â€”â€”
                try:
                    extra = await browser_manager.page.evaluate("""
                        (vid) => {
                            const res = { author_name: '', author_url: '', like_count: 0 };
                            const anchors = Array.from(document.querySelectorAll('a[href*="/video/"]'));
                            let target = null;
                            for (const a of anchors) {
                                const href = a.getAttribute('href') || '';
                                if (href.includes('/video/') && href.includes(vid)) { target = a; break; }
                            }
                            if (!target) return res;
                            let card = target.closest('[class]') || target.parentElement;

                            const authorEl = Array.from(card.querySelectorAll('span, a')).find(el => {
                                const t = (el.textContent || '').trim();
                                return t.startsWith('@') && t.length <= 40;
                            });
                            if (authorEl) {
                                res.author_name = (authorEl.textContent || '').trim().replace(/^@/, '');
                                const au = authorEl.closest('a');
                                if (au && au.href) res.author_url = au.href;
                            }

                            function parseLike(t) {
                                t = (t || '').trim();
                                if (!t || t.includes(':')) return null;
                                const m = t.match(/^(\\d+(?:\\.\\d+)?)([ä¸‡äº¿]?)$/);
                                if (!m) return null;
                                let num = parseFloat(m[1]);
                                if (m[2] === 'ä¸‡') num *= 10000;
                                if (m[2] === 'äº¿') num *= 100000000;
                                return Math.round(num);
                            }
                            let cands = Array.from(card.querySelectorAll('svg + span, div svg + span, div svg ~ span'));
                            cands = cands.concat(Array.from(card.querySelectorAll('span')));
                            for (const el of cands) {
                                const v = parseLike(el.textContent || '');
                                if (v) { res.like_count = v; break; }
                            }
                            return res;
                        }
                    """, aid)
                    if isinstance(extra, dict):
                        video["author_name"] = extra.get("author_name") or ""
                        video["author_url"]  = extra.get("author_url")  or ""
                        video["like_count"]  = int(extra.get("like_count") or 0)
                except Exception:
                    pass

                if data_storage.save_video(video):
                    collected.append(video)
                    collected_ids.add(aid)
                    new_added += 1
        except Exception as e:
            await self._emit_event("debug", f"âŒ RENDER_DATAå…œåº•å¼‚å¸¸: {e}")
        return new_added

    async def _collect_videos_with_smooth_scroll(self, browser_manager, keyword, max_videos):
        """æ•´é¡µä¸æ»‘æ»‘åŠ¨ç‰ˆè§†é¢‘é‡‡é›† - é›†æˆæ™ºèƒ½åº•éƒ¨æ£€æµ‹"""
        videos = []
        collected_urls = set()
        scroll_attempts = 0
        max_scroll_attempts = 30
        no_new_count = 0  # è¿ç»­æ— æ–°è§†é¢‘è®¡æ•°
        
        await self._emit_event("operation", f"ğŸ¬ å¼€å§‹é‡‡é›†è§†é¢‘ï¼Œç›®æ ‡: {max_videos} ä¸ª")

        while len(videos) < max_videos and scroll_attempts < max_scroll_attempts and no_new_count < 5:
            # é‡‡é›†å½“å‰å¯è§åŒºåŸŸçš„è§†é¢‘
            current_videos = await self._collect_visible_videos(browser_manager, keyword)
            
            new_videos = []
            for video in current_videos:
                if video['video_url'] not in collected_urls:
                    collected_urls.add(video['video_url'])
                    new_videos.append(video)
                    # ç«‹å³ä¿å­˜è§†é¢‘åˆ°æ•°æ®åº“
                    try:
                        from utils.data_storage import data_storage
                        success = data_storage.save_video(video)
                        if success:
                            await self._emit_event("debug", f"âœ… ä¿å­˜è§†é¢‘: {video['video_desc'][:50]}...")
                        else:
                            await self._emit_event("error", f"âŒ ä¿å­˜è§†é¢‘å¤±è´¥: {video['video_url']}")
                    except Exception as e:
                        await self._emit_event("error", f"âŒ ä¿å­˜è§†é¢‘å¼‚å¸¸: {e}")
            
            if new_videos:
                videos.extend(new_videos)
                no_new_count = 0  # é‡ç½®æ— æ–°è§†é¢‘è®¡æ•°
                await self._emit_event("debug", f"ğŸ“¹ æœ¬è½®é‡‡é›†åˆ° {len(new_videos)} ä¸ªæ–°è§†é¢‘ï¼Œæ€»è®¡: {len(videos)}")
            else:
                no_new_count += 1
                await self._emit_event("debug", f"âš ï¸ è¿ç»­ {no_new_count} æ¬¡æœªå‘ç°æ–°è§†é¢‘")
            
            # åªæœ‰å½“è¾¾åˆ°ç›®æ ‡æ•°é‡æ—¶æ‰åœæ­¢
            if len(videos) >= max_videos:
                break
            
            # ğŸ¯ ä½¿ç”¨æ™ºèƒ½åº•éƒ¨æ£€æµ‹
            can_scroll = await self._check_can_scroll_videos(browser_manager)
            if not can_scroll:
                await self._emit_event("debug", "ğŸ“œ å·²æ»‘åŠ¨åˆ°åº•éƒ¨ï¼Œåœæ­¢æ»šåŠ¨")
                break
            
            # æ‰§è¡Œä¸æ»‘æ•´é¡µæ»šåŠ¨
            try:
                await browser_manager.page.evaluate("""
                    () => {
                        const scrollAmount = Math.floor(window.innerHeight * 2.5);
                        window.scrollBy({
                            top: scrollAmount,
                            left: 0,
                            behavior: 'smooth'
                        });
                    }
                """)
                
                # æ™ºèƒ½ç­‰å¾…åŠ è½½ï¼ˆä½¿ç”¨ç»Ÿä¸€çš„pauseæ–¹æ³•ï¼‰
                await self.pause(0.3, 0.5, 'scroll_load')
                
            except Exception as e:
                await self._emit_event("debug", f"âš ï¸ æ»šåŠ¨æ“ä½œå¤±è´¥: {e}")
                no_new_count += 1
            
            scroll_attempts += 1
            
            if await self._check_stop():
                break

        await self._emit_event("operation", f"âœ… è§†é¢‘é‡‡é›†å®Œæˆ: {len(videos)}/{max_videos}")
        return videos[:max_videos]
    async def _check_can_scroll_videos(self, browser_manager):
        """è§†é¢‘é‡‡é›†ä¸“ç”¨åº•éƒ¨æ£€æµ‹ - å¢åŠ ç­‰å¾…æœºåˆ¶"""
        try:
            result = await browser_manager.page.evaluate("""
                () => {
                    const windowHeight = window.innerHeight;
                    const docHeight = Math.max(
                        document.body.scrollHeight,
                        document.documentElement.scrollHeight
                    );
                    const currentScroll = window.pageYOffset || document.documentElement.scrollTop;
                    const buffer = 100;  // åº•éƒ¨ç¼“å†²åŒº
                    
                    return {
                        canScroll: currentScroll + windowHeight < docHeight - buffer,
                        currentScroll: currentScroll,
                        totalHeight: docHeight,
                        windowHeight: windowHeight
                    };
                }
            """)
            
            can_scroll = result.get('canScroll', False)
            
            # å¦‚æœæ£€æµ‹åˆ°åº•éƒ¨ï¼Œå…ˆç­‰å¾…ä¸€æ®µæ—¶é—´çœ‹çœ‹æ˜¯å¦æœ‰æ–°å†…å®¹åŠ è½½
            if not can_scroll:
                await self._emit_event("debug", "ğŸ“œ æ£€æµ‹åˆ°å¯èƒ½åˆ°åº•éƒ¨ï¼Œç­‰å¾…8ç§’ç¡®è®¤æ˜¯å¦æœ‰æ–°è§†é¢‘...")
                
                # è®°å½•å½“å‰çš„é¡µé¢é«˜åº¦å’Œè§†é¢‘æ•°é‡
                initial_height = result.get('totalHeight', 0)
                initial_video_count = await self._get_video_count(browser_manager.page)
                
                # ç­‰å¾…8ç§’ï¼ŒæœŸé—´æ¯2ç§’æ£€æŸ¥ä¸€æ¬¡æ˜¯å¦æœ‰æ–°å†…å®¹
                waited_time = 0
                while waited_time < 8 and not await self._check_stop():
                    await self.pause(1, 1, 'bottom_wait')  # ç­‰å¾…2ç§’
                    waited_time += 1
                    
                    # æ£€æŸ¥é¡µé¢é«˜åº¦æ˜¯å¦æœ‰å˜åŒ–
                    current_result = await browser_manager.page.evaluate("""
                        () => {
                            return Math.max(
                                document.body.scrollHeight,
                                document.documentElement.scrollHeight
                            );
                        }
                    """)
                    
                    # æ£€æŸ¥è§†é¢‘æ•°é‡æ˜¯å¦æœ‰å˜åŒ–
                    current_video_count = await self._get_video_count(browser_manager.page)
                    
                    # å¦‚æœé¡µé¢é«˜åº¦æˆ–è§†é¢‘æ•°é‡æœ‰å˜åŒ–ï¼Œè¯´æ˜æœ‰æ–°å†…å®¹åŠ è½½
                    if current_result > initial_height or current_video_count > initial_video_count:
                        await self._emit_event("debug", f"ğŸ”„ ç­‰å¾…æœŸé—´å‘ç°æ–°è§†é¢‘ï¼é¡µé¢é«˜åº¦: {initial_height} â†’ {current_result}, è§†é¢‘: {initial_video_count} â†’ {current_video_count}")
                        return True  # å¯ä»¥ç»§ç»­æ»šåŠ¨
                
                # ç­‰å¾…8ç§’åä»æ— æ–°å†…å®¹ï¼Œç¡®è®¤åˆ°åº•éƒ¨
                await self._emit_event("debug", "ğŸ“œ ç­‰å¾…8ç§’åä»æ— æ–°è§†é¢‘ï¼Œç¡®è®¤å·²æ»‘åŠ¨åˆ°åº•éƒ¨")
                return False
            
            return True
            
        except Exception as e:
            await self._emit_event("debug", f"âš ï¸ æ£€æŸ¥æ»šåŠ¨çŠ¶æ€å¤±è´¥: {e}")
            return True  # å‡ºé”™æ—¶é»˜è®¤å¯ä»¥æ»šåŠ¨ï¼Œé¿å…è¯¯åœ

    async def _get_video_count(self, page):
        """è·å–å½“å‰è§†é¢‘æ•°é‡"""
        try:
            # å¤šç§é€‰æ‹©å™¨è·å–è§†é¢‘æ•°é‡
            selectors = [
                'li .search-result-card',
                '.search-result-card',
                '[data-e2e*="video-item"]',
                '.video-card',
                'a[href*="/video/"]'
            ]
            
            for selector in selectors:
                try:
                    elements = await page.query_selector_all(selector)
                    if elements:
                        return len(elements)
                except:
                    continue
            return 0
        except:
            return 0
    async def _check_can_scroll(self, browser_manager):
        """æ£€æŸ¥æ˜¯å¦å¯ä»¥ç»§ç»­æ»šåŠ¨ - å¢åŠ ç­‰å¾…æœºåˆ¶"""
        try:
            result = await browser_manager.page.evaluate("""
                () => {
                    const windowHeight = window.innerHeight;
                    const docHeight = Math.max(
                        document.body.scrollHeight,
                        document.documentElement.scrollHeight
                    );
                    const currentScroll = window.pageYOffset || document.documentElement.scrollTop;
                    const buffer = 100;  // åº•éƒ¨ç¼“å†²åŒº
                    
                    return {
                        canScroll: currentScroll + windowHeight < docHeight - buffer,
                        currentScroll: currentScroll,
                        totalHeight: docHeight,
                        windowHeight: windowHeight
                    };
                }
            """)
            
            can_scroll = result.get('canScroll', False)
            
            # å¦‚æœæ£€æµ‹åˆ°åº•éƒ¨ï¼Œå…ˆç­‰å¾…ä¸€æ®µæ—¶é—´çœ‹çœ‹æ˜¯å¦æœ‰æ–°å†…å®¹åŠ è½½
            if not can_scroll:
                await self._emit_event("debug", "ğŸ“œ æ£€æµ‹åˆ°å¯èƒ½åˆ°åº•éƒ¨ï¼Œç­‰å¾…6ç§’ç¡®è®¤...")
                
                # è®°å½•å½“å‰çš„é¡µé¢é«˜åº¦å’Œè¯„è®ºæ•°é‡
                initial_height = result.get('totalHeight', 0)
                initial_comment_count = await self._get_comment_count(browser_manager.page)
                
                # ç­‰å¾…8ç§’ï¼ŒæœŸé—´æ¯2ç§’æ£€æŸ¥ä¸€æ¬¡æ˜¯å¦æœ‰æ–°å†…å®¹
                waited_time = 0
                while waited_time < 6 and not await self._check_stop():
                    await self.pause(1, 1, 'bottom_wait')  # ç­‰å¾…200ç§’
                    waited_time += 1
                    
                    # æ£€æŸ¥é¡µé¢é«˜åº¦æ˜¯å¦æœ‰å˜åŒ–
                    current_result = await browser_manager.page.evaluate("""
                        () => {
                            return Math.max(
                                document.body.scrollHeight,
                                document.documentElement.scrollHeight
                            );
                        }
                    """)
                    
                    # æ£€æŸ¥è¯„è®ºæ•°é‡æ˜¯å¦æœ‰å˜åŒ–
                    current_comment_count = await self._get_comment_count(browser_manager.page)
                    
                    # å¦‚æœé¡µé¢é«˜åº¦æˆ–è¯„è®ºæ•°é‡æœ‰å˜åŒ–ï¼Œè¯´æ˜æœ‰æ–°å†…å®¹åŠ è½½
                    if current_result > initial_height or current_comment_count > initial_comment_count:
                        await self._emit_event("debug", f"ğŸ”„ ç­‰å¾…æœŸé—´å‘ç°æ–°å†…å®¹ï¼é¡µé¢é«˜åº¦: {initial_height} â†’ {current_result}, è¯„è®º: {initial_comment_count} â†’ {current_comment_count}")
                        return True  # å¯ä»¥ç»§ç»­æ»šåŠ¨
                
                # ç­‰å¾…8ç§’åä»æ— æ–°å†…å®¹ï¼Œç¡®è®¤åˆ°åº•éƒ¨
                await self._emit_event("debug", "ğŸ“œ ç­‰å¾…8ç§’åä»æ— æ–°å†…å®¹ï¼Œç¡®è®¤å·²æ»‘åŠ¨åˆ°åº•éƒ¨")
                return False
            
            return True
            
        except Exception as e:
            await self._emit_event("debug", f"âš ï¸ æ£€æŸ¥æ»šåŠ¨çŠ¶æ€å¤±è´¥: {e}")
            return True  # å‡ºé”™æ—¶é»˜è®¤å¯ä»¥æ»šåŠ¨ï¼Œé¿å…è¯¯åœ

    async def _start_comment_api_listener(self, browser_manager):
        """å¯åŠ¨è¯„è®ºæ¥å£ç›‘å¬ï¼ˆä¿®å¤awaité—®é¢˜ï¼‰"""
        async def handle_comment_response(response):
            try:
                url = response.url
                # åŒæ—¶å…è®¸fetchå’Œxhrç±»å‹çš„è¯·æ±‚
                if "comment" in url.lower() and response.request.resource_type in ["xhr", "fetch"]:
                    if response.status == 200:
                        data = await response.json()
                        # ä¿®å¤ï¼šæ·»åŠ await
                        comments = await self._extract_comments_from_api(data)
                        if comments:
                            video_url = browser_manager.page.url
                            if video_url not in self.api_comments_cache:
                                self.api_comments_cache[video_url] = []
                            self.api_comments_cache[video_url].extend(comments)
                            await self._emit_event("debug", f"ğŸ“¡ ä»æ¥å£è·å– {len(comments)} æ¡è¯„è®º")
            except Exception as e:
                await self._emit_event("debug", f"âŒ è§£æè¯„è®ºæ¥å£å¤±è´¥: {e}")
        
        self._comment_response_handler = handle_comment_response
        browser_manager.page.on("response", self._comment_response_handler)

    async def _extract_comments_from_api(self, api_data):
        """ä»APIå“åº”æ•°æ®ä¸­æå–è¯„è®ºï¼ˆä¿®å¤å±€éƒ¨å˜é‡å¼•ç”¨ä¸æ¸…æ´—é¡ºåºï¼‰"""
        comments = []
        try:
            # å…¼å®¹ä¸åŒè¿”å›ç»“æ„
            comment_paths = [
                ['comments'],
                ['data', 'comments'],
                ['data'],
                ['list'],
            ]

            comment_list = None
            for path in comment_paths:
                temp = api_data
                for key in path:
                    if isinstance(temp, dict) and key in temp:
                        temp = temp[key]
                    else:
                        temp = None
                        break
                if isinstance(temp, list) and temp:
                    comment_list = temp
                    break

            if not comment_list:
                return comments

            for comment in comment_list:
                try:
                    # å†…å®¹
                    content = self._get_nested_value(comment, ['text', 'content', 'comment'])
                    if not content:
                        continue

                    # ç”¨æˆ·ä¿¡æ¯
                    user_info = self._get_nested_value(comment, ['user', 'author']) or {}
                    username = self._get_nested_value(user_info, ['nickname', 'name']) or ''
                    user_id = self._get_nested_value(user_info, ['uid', 'id']) or ''
                    user_url = f"https://www.douyin.com/user/{user_id}" if user_id else ""

                    # IP å±åœ°ï¼ˆæ¥å£é‡Œå°±æœ‰æ—¶ä¼˜å…ˆç”¨æ¥å£ï¼‰
                    ip_location = self._get_nested_value(comment, ['ip_label', 'location', 'ip_location']) or ''

                    # å‘è¡¨æ—¶é—´æˆ³ï¼ˆæœ‰å°±ç”¨ï¼Œæ²¡æœ‰å†å›é€€åˆ°â€œxxåˆ†é’Ÿå‰â€ç­‰ï¼‰
                    ct_ts = (
                        self._get_nested_value(comment, ['create_time', 'createTime', 'ctime', 'createTs']) or 0
                    )
                    if isinstance(ct_ts, str) and ct_ts.isdigit():
                        ct_ts = int(ct_ts)  # çº¯æ•°å­—å­—ç¬¦ä¸² -> int
                    elif not isinstance(ct_ts, int):
                        ct_ts = 0

                    # å…ˆæ¸…æ´—å‡ºå¹²å‡€æ–‡æœ¬ä¸å¯èƒ½çš„â€œxxåˆ†é’Ÿå‰â€
                    cleaned = self._split_comment_fields(content, username, ip_location)
                    # ä¼˜å…ˆç”¨æ—¶é—´æˆ³â†’äººç±»å¯è¯»ï¼›å¦åˆ™ç”¨æ¸…æ´—å‡ºæ¥çš„â€œxxåˆ†é’Ÿå‰â€
                    ct_str = self._format_time_ago_from_epoch(ct_ts) if ct_ts else cleaned.get('comment_ago', '')

                    comments.append({
                        'username': username,
                        'user_url': user_url,
                        'content': cleaned['text'],
                        'ip_location': cleaned['ip_location'],
                        'source': 'api',
                        'comment_time': ct_str,
                        'comment_ts': int(ct_ts) if ct_ts else data_storage._parse_time_ago_to_epoch(ct_str),
                        'cleaned': True,
                    })
                except Exception:
                    # å•æ¡å¼‚å¸¸è·³è¿‡ï¼Œç»§ç»­è§£æåç»­
                    continue

        except Exception as e:
            await self._emit_event("debug", f"âŒ æå–APIè¯„è®ºæ•°æ®å¤±è´¥: {e}")

        return comments


    async def _filter_comments_by_keywords(self, comments, user_comment_keywords, ip_keywords):
        """æ ¹æ®å…³é”®è¯è¿‡æ»¤è¯„è®º - ä¼˜åŒ–ç‰ˆ"""
        filtered = []
        
        # ä¼˜åŒ–ï¼šé¢„è½¬æ¢IPå…³é”®è¯ä¸ºå°å†™
        ip_keywords_lower = [ipk.lower().strip() for ipk in ip_keywords] if ip_keywords else []
        
        for comment in comments:
            # æ£€æŸ¥æ˜¯å¦å·²ç»æ¸…æ´—è¿‡
            if not comment.get('cleaned'):
                cleaned_data = self._split_comment_fields(
                    comment['content'], 
                    comment['username'], 
                    comment.get('ip_location', '')
                )
            else:
                cleaned_data = {
                    'text': comment['content'],
                    'ip_location': comment.get('ip_location', '')
                }
            
            # ä½¿ç”¨æ¸…æ´—åçš„çº¯è¯„è®ºæ–‡æœ¬è¿›è¡Œå…³é”®è¯åŒ¹é…
            comment_lower = cleaned_data['text'].lower()
            keyword_in_comment = any(kw.lower() in comment_lower for kw in user_comment_keywords)
            
            if not keyword_in_comment:
            # å­è¯„è®ºè‡ªåŠ¨ä¿ç•™ï¼ˆæ–¹æ¡ˆ Bï¼‰
                # if comment.get('source') == 'dom_enhanced' and 'å›å¤' in comment.get('content', ''):
                #     filtered.append(comment)
                #     continue

                continue
            
            # IPåŒ¹é…
            if ip_keywords_lower:
                ip_location = cleaned_data['ip_location']
                ip_location_lower = ip_location.lower() if ip_location else ""
                if not any(ipk in ip_location_lower for ipk in ip_keywords_lower):
                    continue
            
            # æ›´æ–°è¯„è®ºæ•°æ®
            comment['content'] = cleaned_data['text']
            comment['ip_location'] = cleaned_data['ip_location']
            comment['cleaned'] = True
            
            filtered.append(comment)
        
        await self._emit_event("debug", f"ğŸ” å…³é”®è¯è¿‡æ»¤: {len(comments)} -> {len(filtered)}")
        return filtered

    async def _extract_comments_enhanced(self, browser_manager):
        """å¢å¼ºç‰ˆè¯„è®ºæå–"""
        comments = []
        try:
            comment_selectors = [
                'div[data-e2e="comment-item"]',
                '[class*="comment-item"]',
                '.comment-item'
            ]
            
            for selector in comment_selectors:
                try:
                    comment_elements = await browser_manager.page.query_selector_all(selector)
                    if comment_elements:
                        await self._emit_event("debug", f"âœ… æ‰¾åˆ° {len(comment_elements)} ä¸ªè¯„è®ºé¡¹")
                        
                        for element in comment_elements:
                            #await self.pause(0.05, 0.15, 'scroll_comment')  # ä½¿ç”¨ç»Ÿä¸€çš„pauseæ–¹æ³•
                            
                            comment_data = await self._extract_comment_enhanced(element)
                            if comment_data:
                                comments.append(comment_data)
                        break
                except:
                    continue
                    
        except Exception as e:
            await self._emit_event("error", f"âŒ å¢å¼ºç‰ˆè¯„è®ºæå–å¤±è´¥: {e}")
            
        return comments

    async def _extract_comment_enhanced(self, comment_element):
        """ä¿®å¤ç‰ˆè¯„è®ºæ•°æ®æå–"""
        try:
            username = ""
            user_url = ""
            
            user_link_selectors = [
                'a[href^="//www.douyin.com/user/"]',
                'a[href*="/user/"]',
                '[data-e2e="comment-nickname"]',
                '.nickname',
                '.username'
            ]
            
            for selector in user_link_selectors:
                try:
                    user_link = await comment_element.query_selector(selector)
                    if user_link:
                        username = await user_link.inner_text()
                        # â­ å¢å¼ºç‰ˆç”¨æˆ·åæ¸…æ´—ï¼Œå…è®¸éå¸¸è§„ç¬¦å·ã€emojiã€æ—¥éŸ©ã€ç‰¹æ®Šå­—ç­‰
                        try:
                            if username:
                                # å»æ‰ä¸å¯è§å­—ç¬¦
                                username = re.sub(r'[\u200b\u200c\u200d]', '', username)
                                
                                # å»æ‰å‰åæ— æ„ä¹‰ç¬¦å·ï¼Œä½†ä¿ç•™ä¸­é—´çš„
                                username = username.strip().strip('|').strip('Â·').strip('â€¢').strip('-').strip('_').strip('/').strip('\\')

                                # å…è®¸ä¿ç•™ç»å¤§å¤šæ•° Unicode å­—ç¬¦ï¼ˆemojiã€æ—¥éŸ©ã€ç‰¹æ®Šå­—ï¼‰
                                # åªå‰”é™¤æ˜æ˜¾çš„ HTML å™ªå£°
                                #username = username.encode('utf-8', 'ignore').decode('utf-8')
                        except:
                            pass

                        user_url = await user_link.get_attribute('href')
                        if user_url:
                            if user_url.startswith("//"):
                                user_url = "https:" + user_url
                            elif user_url.startswith("/"):
                                user_url = urljoin("https://www.douyin.com", user_url)
                        if username and username.strip():
                            break
                except:
                    continue
            
            if not username.strip():
                try:
                    full_text = await comment_element.inner_text()
                    lines = full_text.split('\n')
                    if lines:
                        raw_name = lines[0].strip()

                        # å…è®¸ Emoji + ä¸­æ–‡ + å­—æ¯ + æ•°å­— + ç‰¹æ®Šç¬¦å·
                        raw_name = re.sub(r'[\u200b\u200c\u200d]', '', raw_name)
                        raw_name = raw_name.encode('utf-8','ignore').decode('utf-8')
                        raw_name = raw_name.strip('|').strip('Â·').strip('â€¢').strip('-').strip('_').strip('/')
                        username = raw_name.strip() or "æœªçŸ¥ç”¨æˆ·"
                except:
                    username = "æœªçŸ¥ç”¨æˆ·"

            import re
            time_text = ""
            try:
                raw = await comment_element.inner_text()
                if raw:
                    m = re.search(r'(åˆšåˆš)|(\d{1,2}\s?(åˆ†é’Ÿå‰|å°æ—¶å‰|å¤©å‰|å‘¨å‰|æœˆå‰|å¹´å‰))', raw)
                    if m:
                        time_text = m.group(0)
            except Exception:
                pass
            raw_comment_text = await self._extract_flattened_text(comment_element)
            if not raw_comment_text:
                return None

            ip_location = await self._extract_ip_location(comment_element)

            return {
                'username': username.strip() if username else "æœªçŸ¥ç”¨æˆ·",
                'user_url': user_url,
                'content': raw_comment_text.strip(),
                'ip_location': ip_location,
                "comment_time": time_text,
                'source': 'dom_enhanced'
            }
            
        except Exception as e:
            await self._emit_event("debug", f"âŒ å¢å¼ºç‰ˆè¯„è®ºæå–å¤±è´¥: {e}")
            return None

    async def _extract_flattened_text(self, element):
        """æ‰å¹³åŒ–æ–‡æœ¬æå–ï¼ˆå¢å¼ºç‰ˆï¼šæ”¯æŒèƒŒæ™¯å›¾è¡¨æƒ…ï¼‰"""
        try:
            text = await element.evaluate("""
                (element) => {
                    // 1. æ™®é€šæ–‡æœ¬èŠ‚ç‚¹
                    const walker = document.createTreeWalker(
                        element,
                        NodeFilter.SHOW_TEXT,
                        null,
                        false
                    );

                    let fullText = '';
                    let node;
                    while ((node = walker.nextNode())) {
                        fullText += node.textContent;
                    }

                    // 2. <img> è¡¨æƒ…ï¼š
                    //    - æœ‰ altï¼šç›´æ¥ç”¨ altï¼ˆä¸€èˆ¬æ˜¯æ­£å¸¸ emoji æˆ– [æš–ç¾Šç¾Š] è¿™ç§ï¼‰
                    //    - æ²¡æœ‰ altï¼šå½“æˆä¸€ä¸ª [è¡¨æƒ…]
                    const imgs = element.querySelectorAll('img');
                    imgs.forEach(img => {
                        const alt = (img.getAttribute('alt') || '').trim();
                        if (alt) {
                            fullText += alt;
                        } else {
                            fullText += '[è¡¨æƒ…]';
                        }
                    });

                    // 3. Douyin è‡ªå®šä¹‰è¡¨æƒ…ï¼š
                    //    ç”¨ sprite èƒŒæ™¯å›¾ + å°å°ºå¯¸ div/span/i ç”»å‡ºæ¥çš„ã€‚
                    //    æˆ‘ä»¬æ£€æµ‹ï¼šæœ‰ backgroundImage ä¸”å°ºå¯¸å¾ˆå°ï¼Œå°±å½“æˆä¸€ä¸ª [è¡¨æƒ…]ã€‚
                    const emojiNodes = element.querySelectorAll('span,div,i');
                    emojiNodes.forEach(node => {
                        const style = window.getComputedStyle(node);
                        const hasBg = style.backgroundImage && style.backgroundImage !== 'none';
                        if (!hasBg) return;

                        const rect = node.getBoundingClientRect();
                        const isSmall =
                            rect.width > 0 && rect.height > 0 &&
                            rect.width <= 32 && rect.height <= 32;  // å°å›¾æ ‡

                        const cls = node.className || '';
                        const isEmojiClass = /emoji|è¡¨æƒ…/i.test(cls);

                        if (isSmall || isEmojiClass) {
                            fullText += '[è¡¨æƒ…]';
                        }
                    });

                    // 4. æ”¶å°¾æ¸…ç†å¤šä½™ç©ºç™½
                    return fullText.replace(/\\s+/g, ' ').trim();
                }
            """)
            return text or ""
        except Exception as e:
            await self._emit_event("debug", f"âŒ æ‰å¹³åŒ–æ–‡æœ¬æå–å¤±è´¥: {e}")
            try:
                return (await element.inner_text()) or ""
            except:
                return ""


    async def _extract_ip_location(self, comment_element):
        """æå–IPå±åœ°ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        try:
            ip_selectors = [
                'span:has(img[src*="loc"])',
                '[class*="ip-label"]',
                '[class*="location"]',
                'span:has(svg)',
            ]
            
            for selector in ip_selectors:
                try:
                    ip_element = await comment_element.query_selector(selector)
                    if ip_element:
                        ip_text = await ip_element.inner_text()
                        if ip_text and ip_text.strip():
                            return ip_text.strip()
                except:
                    continue
                    
            return ""
        except:
            return ""

    async def _merge_comment_data(self, dom_comments, video_url):
        """ä¿®å¤ç‰ˆæ•°æ®åˆå¹¶ - ä½¿ç”¨ä¼ å…¥çš„video_urlä½œä¸ºç¼“å­˜é”®"""
        merged = []
        
        # ä½¿ç”¨ä¼ å…¥çš„video_urlä½œä¸ºç¼“å­˜é”®
        api_comments = self.api_comments_cache.get(video_url, [])
        
        for api_comment in api_comments:
            matched_dom = None
            for dom_comment in dom_comments:
                if (dom_comment['username'] == api_comment['username'] or
                    self._is_similar_content(dom_comment['content'], api_comment['content'])):
                    matched_dom = dom_comment
                    break
            
            merged_comment = {
                'username': api_comment['username'] or (matched_dom['username'] if matched_dom else ''),
                'user_url': matched_dom['user_url'] if matched_dom else api_comment['user_url'],
                'content': api_comment['content'],
                'ip_location': api_comment['ip_location'] or (matched_dom['ip_location'] if matched_dom else '')
            }
            
            if merged_comment['username'] and merged_comment['content']:
                merged.append(merged_comment)
        
        # è¡¥å……DOMä¸­ç‹¬æœ‰çš„è¯„è®º
        for dom_comment in dom_comments:
            already_merged = any(
                self._is_similar_content(dom_comment['content'], merged_comment['content'])
                for merged_comment in merged
            )
            
            if not already_merged and dom_comment['username'] and dom_comment['content']:
                merged.append(dom_comment)
        
        await self._emit_event("debug", f"ğŸ”„ æ•°æ®åˆå¹¶å®Œæˆ: API={len(api_comments)}, DOM={len(dom_comments)}, åˆå¹¶={len(merged)}")
        return merged

    def _is_similar_content(self, content1, content2):
        """åˆ¤æ–­ä¸¤ä¸ªè¯„è®ºå†…å®¹æ˜¯å¦ç›¸ä¼¼"""
        if not content1 or not content2:
            return False
        
        len_diff = abs(len(content1) - len(content2)) / max(len(content1), len(content2))
        words1 = set(content1.lower().split())
        words2 = set(content2.lower().split())
        common_words = words1.intersection(words2)
        
        return len_diff < 0.5 and len(common_words) >= 1

    async def _collect_comments_fallback(self, browser_manager, storage, user_comment_keywords, ip_keywords, video_url, video_desc):
        """é™çº§æ–¹æ¡ˆï¼šæ•´é¡µæ»‘åŠ¨é‡‡é›†è¯„è®º - é›†æˆæ™ºèƒ½åº•éƒ¨æ£€æµ‹"""
        collected = []
        processed = set()
        no_new_count = 0

        ip_keywords_lower = [ipk.lower().strip() for ipk in ip_keywords] if ip_keywords else []

        try:
            # ç­‰å¾…è¯„è®ºåŒºåŸŸåŠ è½½
            await browser_manager.page.wait_for_selector('div[data-e2e="comment-item"]', timeout=5000)
        except:
            await self._emit_event("warning", "âŒ è¯„è®ºåŒºåŸŸåŠ è½½è¶…æ—¶")
            return collected

        scroll_attempts = 0
        max_scroll_attempts = 30

        while no_new_count < 3 and scroll_attempts < max_scroll_attempts and not await self._check_stop():
            try:
                # ğŸ¯ æ£€æŸ¥æ˜¯å¦å¯ä»¥ç»§ç»­æ»šåŠ¨ï¼ˆå¸¦ç­‰å¾…æœºåˆ¶ï¼‰
                can_scroll = await self._check_can_scroll(browser_manager)
                if not can_scroll:
                    await self._emit_event("debug", "ğŸ“œ å·²æ»‘åŠ¨åˆ°åº•éƒ¨ï¼Œåœæ­¢æ»šåŠ¨")
                    break
                
                # é‡‡é›†å½“å‰å¯è§è¯„è®º
                items = await browser_manager.page.query_selector_all('div[data-e2e="comment-item"]')
                new_found = 0

                for item in items:
                    try:
                        comment_data = await self._extract_comment_enhanced(item)
                        if not comment_data:
                            continue

                        cid = f"{comment_data['username']}_{comment_data['content']}"
                        if cid in processed:
                            continue
                        processed.add(cid)
                        new_found += 1

                        cleaned_data = self._split_comment_fields(
                            comment_data['content'], 
                            comment_data['username'], 
                            comment_data.get('ip_location', '')
                        )

                        comment_lower = cleaned_data['text'].lower()
                        if not any(kw.lower() in comment_lower for kw in user_comment_keywords):
                            continue

                        if ip_keywords_lower:
                            ip_location = cleaned_data['ip_location']
                            ip_location_lower = ip_location.lower() if ip_location else ""
                            if not any(ipk in ip_location_lower for ipk in ip_keywords_lower):
                                continue

                        user_data = {
                            "username": comment_data['username'],
                            "user_url": comment_data['user_url'],
                            "comment_text": cleaned_data['text'],
                            "ip_location": cleaned_data['ip_location'],

                            "video_url": video_url,
                            "video_desc": video_desc,
                            "matched_keyword": " ".join(user_comment_keywords),
                            "comment_time":cleaned_data.get('comment_ago',''),
                            "comment_ts": data_storage._parse_time_ago_to_epoch(cleaned_data.get('comment_ago','')),
                        }
                        storage.save_user(user_data)
                        collected.append(user_data)
                        await self._emit_event(
                            "debug", f"ğŸ‘¤ é™çº§é‡‡é›†ç”¨æˆ·: {comment_data['username']} | è¯„è®º: {cleaned_data['text'][:30]}..."
                        )

                    except Exception as e:
                        continue

                # ä½¿ç”¨æ•´é¡µä¸æ»‘æ»‘åŠ¨
                await self._execute_smooth_scroll(browser_manager.page, scroll_attempts)
                    
                await self.pause(0.5, 1, 'scroll_wait')

                no_new_count = 0 if new_found > 0 else no_new_count + 1
                scroll_attempts += 1

            except Exception as e:
                await self._emit_event("warning", f"âš ï¸ é™çº§é‡‡é›†å¤±è´¥: {e}")
                no_new_count += 1
                await self.pause(0.5, 1, 'error_wait')

        return collected

    async def _collect_visible_videos(self, browser_manager, keyword):
        """é‡‡é›†å½“å‰å¯è§åŒºåŸŸçš„è§†é¢‘"""
        videos = []
        try:
            card_selectors = [
                'li .search-result-card',
                '.search-result-card',
                '[data-e2e*="video-item"]',
                '.video-card',
                'a[href*="/video/"]'
            ]
            
            for selector in card_selectors:
                try:
                    elements = await browser_manager.page.query_selector_all(selector)
                    if elements:
                        for element in elements:
                            try:
                                video_data = await self._extract_video_from_element(element, keyword)
                                if video_data and video_data.get('video_url'):
                                    videos.append(video_data)
                            except Exception as e:
                                continue
                                
                        if videos:
                            break
                except:
                    continue
                    
        except Exception as e:
            await self._emit_event("debug", f"âš ï¸ é‡‡é›†å¯è§è§†é¢‘å¤±è´¥: {e}")
            
        return videos

    async def _extract_video_from_element(self, element, keyword):
        """ä»è§†é¢‘å…ƒç´ ä¸­æå–æ•°æ®ï¼ˆè¡¥å……ä½œè€…ä¸»é¡µ + ä½œè€…æ˜µç§° + ç‚¹èµæ•°ï¼‰"""
        try:
            # è§†é¢‘é“¾æ¥
            link_el = await element.query_selector('a[href*="/video/"]')
            if not link_el:
                return None

            href = await link_el.get_attribute('href')
            if not href:
                return None

            video_url = urljoin("https://www.douyin.com", href)

            # ä½œè€…æ˜µç§° + ä½œè€…ä¸»é¡µ
            # ä½œè€…æ˜µç§° + ä½œè€…ä¸»é¡µï¼ˆæ ‡é¢˜æ™ºèƒ½æ¨æ–­ï¼šå–â€œæ—¶é—´èŠ‚ç‚¹â€å‰çš„æœ€è¿‘åå­—ï¼‰
            # ä½œè€…æ˜µç§° & ä½œè€…ä¸»é¡µ
            author_name = ""
            author_url = ""
            try:
                # å…ˆå°è¯•ä» DOM é‡Œæ‹¿çœŸå®ä½œè€…èŠ‚ç‚¹ï¼ˆå¦‚æœæœ‰ï¼‰
                author_link_selectors = [
                    'a[href*="/user/"]',
                    '[data-e2e*="search-user-name"]',
                    '[data-e2e*="video-author-name"]',
                ]
                for sel in author_link_selectors:
                    try:
                        a_el = await element.query_selector(sel)
                        if not a_el:
                            continue
                        txt = await a_el.inner_text()
                        if txt:
                            author_name = txt.strip()
                        href_user = await a_el.get_attribute("href")
                        if href_user:
                            author_url = urljoin("https://www.douyin.com", href_user)
                        break
                    except:
                        continue
            except:
                pass

            # å¦‚æœ DOM é‡Œæ‹¿ä¸åˆ°ä½œè€…ï¼Œåˆ™æŒ‰â€œæ—¶é—´èŠ‚ç‚¹å‰æœ€è¿‘çš„äººåâ€è§„åˆ™ä»æ–‡æœ¬é‡Œæ¨æ–­
            if not author_name:
                try:
                    full_text = await element.inner_text() or ""
                    full_text = full_text.strip()

                    # æå–å‘å¸ƒæ—¶é—´æ–‡æœ¬ï¼Œä¾‹å¦‚ â€œ3æœˆå‰â€
                    publish_time = await self._extract_video_publish_time(element)
                    name_text = ""

                    if publish_time and publish_time in full_text:
                        idx = full_text.rfind(publish_time)
                        # åœ¨æ—¶é—´èŠ‚ç‚¹å‰é¢æˆªå–ä¸€æ®µçª—å£æ–‡æœ¬
                        window_text = full_text[max(0, idx - 40): idx]

                        # â‘  ä¼˜å…ˆæ‰¾ @åå­—
                        at_matches = [m.group(1) for m in re.finditer(
                            r'@((?:[\w\u4e00-\u9fa5]|[^\s\w]){1,20})',
                            window_text
                        )]
                        if at_matches:
                            name_text = at_matches[-1]

                        # â‘¡ æ²¡æœ‰ @ï¼Œå†æ‰¾è¿ç»­çš„ä¸­æ–‡/å­—æ¯æ•°å­—å
                        if not name_text:
                            cn_matches = [m.group(1) for m in re.finditer(
                                r'([\u4e00-\u9fa5A-Za-z0-9Â·]{2,12})',
                                window_text
                            )]
                            if cn_matches:
                                name_text = cn_matches[-1]

                    if name_text:
                        author_name = name_text.strip()

                    if not author_name:
                        author_name = "æœªçŸ¥ä½œè€…"

                    # è¿™é‡Œæ²¡æœ‰çœŸå®ä¸»é¡µé“¾æ¥ï¼Œåªèƒ½ç•™ç©ºï¼Œè®©å‰ç«¯åªæ˜¾ç¤ºåå­—ä¸è·³è½¬
                    if not author_url:
                        author_url = ""
                except:
                    if not author_name:
                        author_name = "æœªçŸ¥ä½œè€…"
                    if not author_url:
                        author_url = ""



            # ç‚¹èµæ•°ï¼ˆè§£ææ•°å­— / ä¸‡ / äº¿ï¼‰
            like_count = 0
            try:
                span_texts = await element.eval_on_selector_all(
                    "span",
                    "els => els.map(e => e.textContent.trim())"
                )
                import re as _re
                like_text = ""
                for t in span_texts:
                    if _re.fullmatch(r"[0-9]+(?:\.[0-9]+)?[ä¸‡äº¿]?", t):
                        like_text = t
                        break

                if like_text:
                    if like_text.endswith("ä¸‡"):
                        like_count = int(float(like_text[:-1]) * 10000)
                    elif like_text.endswith("äº¿"):
                        like_count = int(float(like_text[:-1]) * 100000000)
                    else:
                        like_count = int(like_text)
            except:
                like_count = 0

            # æ ‡é¢˜ä¸å‘å¸ƒæ—¶é—´
            title = await self._extract_video_title(element)
            publish_time = await self._extract_video_publish_time(element)
            publish_ts = data_storage._parse_time_ago_to_epoch(publish_time) if publish_time else 0

            return {
                "video_url": video_url,
                "video_desc": title or "æ— æ ‡é¢˜",
                "keyword": keyword,
                "publish_time": publish_time,
                "publish_ts": publish_ts,
                "author_name": author_name,
                "author_url": author_url,
                "like_count": like_count,
            }
        except Exception:
            return None

    async def _extract_video_detail_from_page(self, browser_manager):
        """
        è¿›å…¥è§†é¢‘è¯¦æƒ…é¡µåæå–ï¼š
        - like_count / comment_count / collect_count
        - author_name / author_url
        - video_desc
        å…ˆ JSONï¼Œå DOM å…œåº•ï¼›ä¸ä¾èµ–éšæœºç±»å
        """
        page = browser_manager.page

        # -------- 1) è¯¦æƒ…é¡µ URL è§„èŒƒåŒ–å¹¶è·³è½¬ --------
        try:
            cur_url = page.url or ""
        except Exception:
            cur_url = ""
        clean_url = self._normalize_video_url(cur_url)
        if clean_url and clean_url != cur_url:
            await page.goto(clean_url, timeout=60000)

        # ç­‰å¾…è¯¦æƒ…ä¸»ä½“å‡ºç°ï¼›8s å†…æ²¡å‡ºæ¥ç›´æ¥è¿”å›é»˜è®¤ï¼ˆé¿å…å¡æ­»ï¼‰
        try:
            await page.wait_for_selector('script#RENDER_DATA, div[data-e2e="user-info"]', timeout=8000)
        except Exception:
            pass

        # é¢„ç½®é»˜è®¤
        author_name = "æœªçŸ¥ä½œè€…"
        author_url = ""
        like_count = 0
        comment_count = 0
        collect_count = 0

        # -------- 2) ä¼˜å…ˆä» RENDER_DATA JSON è§£æï¼ˆæœ€ç¨³ï¼‰--------
        try:
            script_el = await page.query_selector('script#RENDER_DATA')
            if script_el:
                raw = await script_el.inner_text()
                if raw:
                    import json, urllib.parse
                    # RENDER_DATA æ˜¯ URL ç¼–ç çš„ JSONï¼Œéœ€è¦å…ˆè§£ç 
                    decoded = urllib.parse.unquote(raw)
                    data = json.loads(decoded)

                    # å…¼å®¹å¤šç‰ˆæœ¬ç»“æ„ï¼šåœ¨æ‰€æœ‰èŠ‚ç‚¹é‡Œå¹¿åº¦æœç´¢éœ€è¦å­—æ®µ
                    def _walk(obj):
                        if isinstance(obj, dict):
                            for k, v in obj.items():
                                yield k, v
                                if isinstance(v, (dict, list)):
                                    yield from _walk(v)
                        elif isinstance(obj, list):
                            for it in obj:
                                yield from _walk(it)

                    # é€é¡¹æå–
                    for k, v in _walk(data):
                        # ä½œè€…
                        if author_name == "æœªçŸ¥ä½œè€…" and isinstance(v, dict):
                            # å¸¸è§å­—æ®µï¼šauthor/authorInfo/nickname/name
                            for kk in ("author", "authorInfo", "user", "userInfo"):
                                if kk in v and isinstance(v[kk], dict):
                                    nn = v[kk].get("nickname") or v[kk].get("name")
                                    hu = v[kk].get("secUid") or v[kk].get("sec_uid") or v[kk].get("uid") or v[kk].get("id")
                                    if nn:
                                        author_name = str(nn).strip()
                                    if hu:
                                        # ä¼˜å…ˆç”¨ secUid ç”Ÿæˆç¨³å®šä¸»é¡µ
                                        author_url = f"https://www.douyin.com/user/{hu}"
                                    if author_name != "æœªçŸ¥ä½œè€…" and author_url:
                                        break

                        # ç‚¹èµ/è¯„è®º/æ”¶è—
                        if isinstance(v, dict):
                            lc = v.get("diggCount") or v.get("likeCount") or v.get("statistics", {}).get("diggCount")
                            cc = v.get("commentCount") or v.get("statistics", {}).get("commentCount")
                            sc = v.get("collectCount") or v.get("favoriteCount") or v.get("statistics", {}).get("collectCount")
                            if isinstance(lc, (int, float)) and lc > like_count:
                                like_count = int(lc)
                            if isinstance(cc, (int, float)) and cc > comment_count:
                                comment_count = int(cc)
                            if isinstance(sc, (int, float)) and sc > collect_count:
                                collect_count = int(sc)

        except Exception:
            # JSON è§£æå¤±è´¥æ—¶ï¼Œè¿›å…¥ DOM å…œåº•
            pass

        # -------- 3) DOM å…œåº•ï¼ˆä¸ä¾èµ–éšæœº classï¼›ä»æŒ‰é’®é™„è¿‘å–æ•°å­—ï¼‰--------
        async def _dom_pick_by_icon(icon_keywords):
            """
            åœ¨åŒ…å«ç‚¹èµ/è¯„è®º/æ”¶è—å›¾æ ‡çš„æŒ‰é’®é™„è¿‘å–æ•°å­—ï¼›icon_keywords å¦‚ ["èµ","like"] / ["è¯„","comment"] / ["è—","collect"]
            """
            try:
                # æ‰¾åˆ°æ‰€æœ‰æŒ‰é’®/å®¹å™¨
                nodes = await page.query_selector_all("button, a, div, span")
                import re
                for n in nodes:
                    try:
                        html = (await n.inner_html()) or ""
                        text = (await n.inner_text()) or ""
                        if not html:
                            continue
                        if any(kw in html for kw in icon_keywords):
                            m = re.search(r"([\d\.]+ä¸‡|[\d\.]+w|[\d,]+)", text.replace(",", ""))
                            if m:
                                return self._parse_count_text(m.group(1))
                    except Exception:
                        continue
            except Exception:
                pass
            return 0

        if like_count == 0:
            like_count = await _dom_pick_by_icon(["èµ", "like", "dianzan", "svg"])
        if comment_count == 0:
            comment_count = await _dom_pick_by_icon(["è¯„", "comment", "pinglun", "svg"])
        if collect_count == 0:
            collect_count = await _dom_pick_by_icon(["è—", "collect", "shoucang", "favorite", "svg"])

        # -------- 4) ä½œè€…ä¿¡æ¯ DOM å…œåº•ï¼ˆå¤šçº§é€‰æ‹©å™¨ï¼›ç»Ÿä¸€ç»å¯¹ URLï¼‰--------
        if author_name == "æœªçŸ¥ä½œè€…" or not author_url:
            author_selectors = [
                'div[data-e2e="user-info"] a[href*="/user/"]',
                'div.OMAnIChG a[href*="/user/"]',
                'div.OwMAhChG a[href*="/user/"]',
                'div.ChsTMt34 a[href*="/user/"]',
                'a[href^="//www.douyin.com/user/"]',
                'a[href^="/user/"]',
            ]
            for sel in author_selectors:
                try:
                    a = await page.query_selector(sel)
                    if not a:
                        continue
                    name = (await a.inner_text()) or ""
                    href = await a.get_attribute("href") or ""
                    if name.strip():
                        author_name = name.strip()
                    if href:
                        if href.startswith("//"):
                            author_url = "https:" + href
                        elif href.startswith("/"):
                            author_url = "https://www.douyin.com" + href
                        else:
                            author_url = href
                    if author_name != "æœªçŸ¥ä½œè€…" and author_url:
                        break
                except Exception:
                    continue

        # -------- 5) è§†é¢‘æ–‡æ¡ˆï¼ˆå¤ç”¨ä½ ç°æœ‰çš„å¢å¼ºç‰ˆï¼‰--------
        try:
            video_desc = await self._read_video_desc_enhanced(browser_manager)
        except Exception:
            video_desc = ""

        return {
            "video_desc": video_desc or "",
            "author_name": author_name or "æœªçŸ¥ä½œè€…",
            "author_url": author_url or "",
            "like_count": int(like_count) if like_count else 0,
            "comment_count": int(comment_count) if comment_count else 0,
            "collect_count": int(collect_count) if collect_count else 0,
        }


    async def _extract_video_publish_time(self, element) -> str:
        """
        ä»å¡ç‰‡å†…æ–‡ä¸­ä»¥æ–‡æœ¬æ–¹å¼æŠ“ â€œXåˆ†é’Ÿå‰/å°æ—¶å‰/å¤©å‰/å‘¨å‰/æœˆå‰/å¹´å‰/åˆšåˆšâ€
        æŠ“ä¸åˆ°è¿”å›ç©ºä¸²ã€‚
"""
        try:
            text = await element.inner_text()
            if not text:
                return ""
            import re
            m=re.search(r'(\d{1,2}\s?(åˆ†é’Ÿå‰|å°æ—¶å‰|å¤©å‰|å‘¨å‰|æœˆå‰|å¹´å‰|åˆšåˆš))', text)
            return m.group(0) if m else ""
        except Exception:
            return ""
    async def _extract_video_title(self, element):
        """æå–è§†é¢‘æ ‡é¢˜"""
        try:
            title_text = await element.inner_text() or ""
            # ä¿®å¤ï¼šåªç§»é™¤ä¸å¯è§æ§åˆ¶å­—ç¬¦ï¼Œä¸ç§»é™¤ emoji/ç‰¹æ®Šç¬¦å·
            title_text = re.sub(r'[\u200b\u200c\u200d]', '', title_text)

            if title_text and len(title_text.strip()) > 10:
                return title_text.strip()[:100]
            
            title_selectors = [
                '[data-e2e*="video-desc"]',
                '.video-desc',
                '.title',
                'div[class*="desc"]',
                'span[class*="desc"]'
            ]
            
            for selector in title_selectors:
                try:
                    title_el = await element.query_selector(selector)
                    if title_el:
                        text = await title_el.inner_text()
                        if text and text.strip():
                            return text.strip()[:100]
                except:
                    continue
                    
            return ""
            
        except:
            return ""
    def _normalize_video_url(self, url: str) -> str:
        """
        æ¸…æ´— Douyin è§†é¢‘ URLï¼šå»æ‰ modeFromã€share_token ç­‰å½±å“æ¸²æŸ“çš„å°¾å‚æ•°ï¼Œ
        ç»Ÿä¸€ä¸º https://www.douyin.com/video/{id}
        """
        import re
        m = re.search(r"/video/(\d+)", url or "")
        if not m:
            return url
        vid = m.group(1)
        return f"https://www.douyin.com/video/{vid}"

    async def _read_video_desc_enhanced(self, browser_manager) -> str:
        """å¢å¼ºç‰ˆè§†é¢‘æ–‡æ¡ˆè¯»å–"""
        data_selectors = [
            '[data-e2e="video-desc"]',
            '[data-e2e*="desc"]',
            '[data-e2e*="title"]'
        ]
        
        for selector in data_selectors:
            try:
                element = await browser_manager.page.query_selector(selector)
                if element:
                    text = await element.inner_text()
                    if text and text.strip():
                        await self._emit_event("debug", f"âœ… é€šè¿‡dataå±æ€§è·å–æ–‡æ¡ˆ: {text[:50]}...")
                        return text.strip()
            except:
                continue
        
        try:
            meta_element = await browser_manager.page.query_selector('meta[property="og:description"]')
            if meta_element:
                content = await meta_element.get_attribute('content')
                if content and content.strip():
                    await self._emit_event("debug", f"âœ… é€šè¿‡OG metaè·å–æ–‡æ¡ˆ: {content[:50]}...")
                    return content.strip()
        except:
            pass
        
        try:
            json_data = await self._extract_desc_from_json(browser_manager)
            if json_data:
                await self._emit_event("debug", f"âœ… é€šè¿‡JSONè·å–æ–‡æ¡ˆ: {json_data[:50]}...")
                return json_data
        except:
            pass
        
        try:
            title = await browser_manager.page.title()
            if title and "æŠ–éŸ³" not in title:
                return title
        except:
            pass
            
        return "æ— æè¿°"

    async def _extract_desc_from_json(self, browser_manager):
        """ä»é¡µé¢JSONæ•°æ®ä¸­æå–è§†é¢‘æè¿°"""
        try:
            script_selectors = [
                'script#RENDER_DATA',
                'script[type="application/json"]',
                'script[data-react-helmet]'
            ]
            
            for selector in script_selectors:
                try:
                    script_element = await browser_manager.page.query_selector(selector)
                    if script_element:
                        script_content = await script_element.inner_text()
                        if script_content:
                            import urllib.parse
                            decoded_content = urllib.parse.unquote(script_content)
                            
                            # æŸ¥æ‰¾å¸¸è§çš„æè¿°å­—æ®µ
                            desc_patterns = [
                                r'"desc":"([^"]+)"',
                                r'"aweme_desc":"([^"]+)"',
                                r'"video_desc":"([^"]+)"',
                                r'"title":"([^"]+)"'
                            ]
                            
                            for pattern in desc_patterns:
                                matches = re.findall(pattern, decoded_content)
                                if matches and matches[0]:
                                    return matches[0]
                except:
                    continue
                    
            return None
        except:
            return None

    async def _quick_wait_for_page_load(self, browser_manager, timeout=10):
        """å¿«é€Ÿç­‰å¾…é¡µé¢åŠ è½½å®Œæˆ"""
        try:
            await self._emit_event("operation", "â³ å¿«é€Ÿç­‰å¾…é¡µé¢åŠ è½½...")
            
            start_time = asyncio.get_event_loop().time()
            
            try:
                await browser_manager.page.wait_for_load_state('domcontentloaded', timeout=5000)
                await self._emit_event("debug", "âœ… DOMå†…å®¹åŠ è½½å®Œæˆ")
            except Exception as e:
                await self._emit_event("debug", f"âš ï¸ DOMåŠ è½½ç­‰å¾…è¶…æ—¶: {e}")
            
            search_indicators = [
                '[data-e2e="search-input"]',
                '.search-container',
                '.search-result',
                'body'
            ]
            
            element_found = False
            for selector in search_indicators:
                try:
                    await browser_manager.page.wait_for_selector(selector, timeout=3000)
                    element_found = True
                    await self._emit_event("debug", f"âœ… å…³é”®å…ƒç´ åŠ è½½: {selector}")
                    break
                except:
                    continue
            
            await self.pause(0.5, 1, 'page_load')  # ä½¿ç”¨ç»Ÿä¸€çš„pauseæ–¹æ³•
            
            content_ready = await self._quick_check_page_content(browser_manager)
            if not content_ready:
                await self._emit_event("debug", "âš ï¸ é¡µé¢å†…å®¹å¯èƒ½æœªå®Œå…¨åŠ è½½ï¼Œä½†ç»§ç»­æ‰§è¡Œ")
            
            elapsed = asyncio.get_event_loop().time() - start_time
            await self._emit_event("debug", f"â±ï¸ é¡µé¢åŠ è½½è€—æ—¶: {elapsed:.1f}ç§’")
            
        except Exception as e:
            await self._emit_event("debug", f"âš ï¸ å¿«é€Ÿç­‰å¾…è¿‡ç¨‹ä¸­å‡ºé”™: {e}")

    async def _quick_check_page_content(self, browser_manager):
        """å¿«é€Ÿæ£€æŸ¥é¡µé¢å†…å®¹"""
        try:
            content_check = await browser_manager.page.evaluate("""
                () => {
                    return {
                        hasBody: !!document.body,
                        bodyChildren: document.body ? document.body.children.length : 0,
                        readyState: document.readyState
                    };
                }
            """)
            
            is_ready = content_check['hasBody'] and content_check['bodyChildren'] > 0
            
            await self._emit_event("debug", 
                f"ğŸ“Š å¿«é€Ÿæ£€æŸ¥: Bodyå­˜åœ¨={content_check['hasBody']}, "
                f"å­å…ƒç´ ={content_check['bodyChildren']}, "
                f"çŠ¶æ€={content_check['readyState']}")
            
            return is_ready
            
        except Exception as e:
            await self._emit_event("debug", f"âš ï¸ å¿«é€Ÿå†…å®¹æ£€æŸ¥å¤±è´¥: {e}")
            return True

    async def _stop_comment_api_listener(self, browser_manager):
        """åœæ­¢è¯„è®ºæ¥å£ç›‘å¬"""
        if hasattr(self, '_comment_response_handler'):
            browser_manager.page.remove_listener("response", self._comment_response_handler)

    def _get_nested_value(self, data, keys):
        """å®‰å…¨è·å–åµŒå¥—å­—å…¸çš„å€¼"""
        try:
            current = data
            for key in keys:
                if isinstance(current, dict) and key in current:
                    current = current[key]
                else:
                    return None
            return current
        except:
            return None

    async def enrich_video_detail(self, browser_manager, video_url: str) -> dict:
        """
        æ‰“å¼€è§†é¢‘è¯¦æƒ…é¡µå¹¶é‡‡é›†ï¼š
        - ä½œè€…æ˜µç§°
        - ä½œè€…ä¸»é¡µ
        - ç‚¹èµæ•°
        - è¯„è®ºæ•°
        - æ”¶è—æ•°
        - è§†é¢‘æ ‡é¢˜
        """
        page = browser_manager.page

        try:
            await page.goto(video_url, wait_until="domcontentloaded")
            await asyncio.sleep(1.2)

            # è§†é¢‘æ ‡é¢˜
            title = ""
            try:
                title_el = await page.query_selector('[data-e2e="video-desc"]')
                if title_el:
                    title = (await title_el.inner_text()).strip()
            except:
                pass

            # ä½œè€…ä¸»é¡µ
            author_name, author_url = "", ""
            try:
                user_block = await page.query_selector('a[href*="/user/"]')
                if user_block:
                    author_name = (await user_block.inner_text()).strip()
                    href = await user_block.get_attribute("href")
                    if href:
                        author_url = "https:" + href if href.startswith("//") else href
            except:
                pass

            # ç‚¹èµ/è¯„è®º/æ”¶è— æ•°
            def parse_count(s):
                import re
                if not s:
                    return 0
                s = s.replace(" ", "")
                if s.endswith("ä¸‡"):
                    return int(float(s[:-1]) * 10000)
                if s.endswith("äº¿"):
                    return int(float(s[:-1]) * 100000000)
                m = re.findall(r"\d+", s)
                return int(m[0]) if m else 0

            like = comment = collect = 0
            try:
                spans = await page.eval_on_selector_all(
                    "span",
                    "els => els.map(e => e.textContent.trim())"
                )
                for t in spans:
                    if "ç‚¹èµ" in t:
                        like = parse_count(t.replace("ç‚¹èµ", ""))
                    elif "è¯„è®º" in t:
                        comment = parse_count(t.replace("è¯„è®º", ""))
                    elif "æ”¶è—" in t:
                        collect = parse_count(t.replace("æ”¶è—", ""))
            except:
                pass

            return {
                "video_url": video_url,
                "video_desc": title or "",
                "author_name": author_name or "æœªçŸ¥ä½œè€…",
                "author_url": author_url or "",
                "like_count": like,
                "comment_count": comment,
                "collect_count": collect
            }

        except Exception as e:
            print(f"é‡‡é›†è§†é¢‘è¯¦æƒ…å¤±è´¥: {e}")
            return {}


